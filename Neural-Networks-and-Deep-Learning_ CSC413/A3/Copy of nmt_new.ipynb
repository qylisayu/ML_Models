{"cells":[{"cell_type":"markdown","metadata":{"id":"TjPTaRB4mpCd"},"source":["# Colab FAQ\n","\n","For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n","\n","You need to use the colab GPU for this assignment by selecting:\n","\n","> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"]},{"cell_type":"markdown","metadata":{"id":"s9IS9B9-yUU5"},"source":["# Setup PyTorch\n","\n","All files will be stored at /content/csc421/a3/ folder\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Z-6MQhMOlHXD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647557192600,"user_tz":240,"elapsed":3481,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}},"outputId":"d20c1ec8-d26c-4b86-d911-d26df4f519d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n","/content/content/csc421/a3/content/csc421/a3\n"]}],"source":["######################################################################\n","# Setup python environment and change the current working directory\n","######################################################################\n","!pip install Pillow\n","%mkdir -p ./content/csc421/a3/\n","%cd ./content/csc421/a3"]},{"cell_type":"markdown","metadata":{"id":"9DaTdRNuUra7"},"source":["# Helper code"]},{"cell_type":"markdown","metadata":{"id":"4BIpGwANoQOg"},"source":["## Utility functions"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"D-UJHBYZkh7f","executionInfo":{"status":"ok","timestamp":1647557193384,"user_tz":240,"elapsed":790,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}}},"outputs":[],"source":["%matplotlib inline\n","\n","import os\n","import pdb\n","import argparse\n","import pickle as pkl\n","from pathlib import Path\n","\n","from collections import defaultdict\n","\n","import numpy as np\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","from six.moves.urllib.request import urlretrieve\n","import tarfile\n","import pickle\n","import sys\n","\n","\n","def get_file(\n","    fname, origin, untar=False, extract=False, archive_format=\"auto\", cache_dir=\"data\"\n","):\n","    datadir = os.path.join(cache_dir)\n","    if not os.path.exists(datadir):\n","        os.makedirs(datadir)\n","\n","    if untar:\n","        untar_fpath = os.path.join(datadir, fname)\n","        fpath = untar_fpath + \".tar.gz\"\n","    else:\n","        fpath = os.path.join(datadir, fname)\n","\n","    print(fpath)\n","    if not os.path.exists(fpath):\n","        print(\"Downloading data from\", origin)\n","\n","        error_msg = \"URL fetch failure on {}: {} -- {}\"\n","        try:\n","            try:\n","                urlretrieve(origin, fpath)\n","            except URLError as e:\n","                raise Exception(error_msg.format(origin, e.errno, e.reason))\n","            except HTTPError as e:\n","                raise Exception(error_msg.format(origin, e.code, e.msg))\n","        except (Exception, KeyboardInterrupt) as e:\n","            if os.path.exists(fpath):\n","                os.remove(fpath)\n","            raise\n","\n","    if untar:\n","        if not os.path.exists(untar_fpath):\n","            print(\"Extracting file.\")\n","            with tarfile.open(fpath) as archive:\n","                archive.extractall(datadir)\n","        return untar_fpath\n","\n","    if extract:\n","        _extract_archive(fpath, datadir, archive_format)\n","\n","    return fpath\n","\n","\n","class AttrDict(dict):\n","    def __init__(self, *args, **kwargs):\n","        super(AttrDict, self).__init__(*args, **kwargs)\n","        self.__dict__ = self\n","\n","\n","def to_var(tensor, cuda):\n","    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n","\n","    Arguments:\n","        tensor: A Tensor object.\n","        cuda: A boolean flag indicating whether to use the GPU.\n","\n","    Returns:\n","        A Variable object, on the GPU if cuda==True.\n","    \"\"\"\n","    if cuda:\n","        return Variable(tensor.cuda())\n","    else:\n","        return Variable(tensor)\n","\n","\n","def create_dir_if_not_exists(directory):\n","    \"\"\"Creates a directory if it doesn't already exist.\"\"\"\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","\n","\n","def save_loss_plot(train_losses, val_losses, opts):\n","    \"\"\"Saves a plot of the training and validation loss curves.\"\"\"\n","    plt.figure()\n","    plt.plot(range(len(train_losses)), train_losses)\n","    plt.plot(range(len(val_losses)), val_losses)\n","    plt.title(\"BS={}, nhid={}\".format(opts.batch_size, opts.hidden_size), fontsize=20)\n","    plt.xlabel(\"Epochs\", fontsize=16)\n","    plt.ylabel(\"Loss\", fontsize=16)\n","    plt.xticks(fontsize=14)\n","    plt.yticks(fontsize=14)\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(opts.checkpoint_path, \"loss_plot.pdf\"))\n","    plt.close()\n","\n","\n","def save_loss_comparison_gru(l1, l2, o1, o2, fn, s=500):\n","    \"\"\"Plot comparison of training and val loss curves from GRU runs.\n","\n","    Arguments:\n","        l1: Tuple of lists containing training / val losses for model 1.\n","        l2: Tuple of lists containing training / val losses for model 2.\n","        o1: Options for model 1.\n","        o2: Options for model 2.\n","        fn: Output file name.\n","        s: Number of training iterations to average over.\n","    \"\"\"\n","    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n","    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n","\n","    plt.figure()\n","\n","    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n","\n","    ax[0].plot(range(len(mean_l1)), mean_l1, label=\"ds=\" + o1.data_file_name)\n","    ax[0].plot(range(len(mean_l2)), mean_l2, label=\"ds=\" + o2.data_file_name)\n","    ax[0].title.set_text(\"Train Loss | GRU Hidden Size = {}\".format(o2.hidden_size))\n","\n","    # Validation losses are assumed to be by epoch\n","    ax[1].plot(range(len(l1[1])), l1[1], label=\"ds=\" + o1.data_file_name)\n","    ax[1].plot(range(len(l2[1])), l2[1], label=\"ds=\" + o2.data_file_name)\n","    ax[1].title.set_text(\"Val Loss | GRU Hidden Size = {}\".format(o2.hidden_size))\n","\n","    ax[0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n","    ax[0].set_ylabel(\"Loss\", fontsize=10)\n","    ax[1].set_xlabel(\"Epochs\", fontsize=10)\n","    ax[1].set_ylabel(\"Loss\", fontsize=10)\n","    ax[0].legend(loc=\"upper right\")\n","    ax[1].legend(loc=\"upper right\")\n","\n","    fig.suptitle(\"GRU Performance by Dataset\", fontsize=14)\n","    plt.tight_layout()\n","    fig.subplots_adjust(top=0.85)\n","    plt.legend()\n","\n","    plt_path = \"./loss_plot_{}.pdf\".format(fn)\n","    plt.savefig(plt_path)\n","    print(f\"Plot saved to: {Path(plt_path).resolve()}\")\n","\n","\n","def save_loss_comparison_by_dataset(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n","    \"\"\"Plot comparison of training and validation loss curves from all four\n","    runs in Part 3, comparing by dataset while holding hidden size constant.\n","\n","    Models within each pair (l1, l2) and (l3, l4) have the same hidden sizes.\n","\n","    Arguments:\n","        l1: Tuple of lists containing training / val losses for model 1.\n","        l2: Tuple of lists containing training / val losses for model 2.\n","        l3: Tuple of lists containing training / val losses for model 3.\n","        l4: Tuple of lists containing training / val losses for model 4.\n","        o1: Options for model 1.\n","        o2: Options for model 2.\n","        o3: Options for model 3.\n","        o4: Options for model 4.\n","        fn: Output file name.\n","        s: Number of training iterations to average over.\n","    \"\"\"\n","    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n","    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n","    mean_l3 = [np.mean(l3[0][i * s : (i + 1) * s]) for i in range(len(l3[0]) // s)]\n","    mean_l4 = [np.mean(l4[0][i * s : (i + 1) * s]) for i in range(len(l4[0]) // s)]\n","\n","    plt.figure()\n","    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n","\n","    ax[0][0].plot(range(len(mean_l1)), mean_l1, label=\"ds=\" + o1.data_file_name)\n","    ax[0][0].plot(range(len(mean_l2)), mean_l2, label=\"ds=\" + o2.data_file_name)\n","    ax[0][0].title.set_text(\n","        \"Train Loss | Model Hidden Size = {}\".format(o1.hidden_size)\n","    )\n","\n","    # Validation losses are assumed to be by epoch\n","    ax[0][1].plot(range(len(l1[1])), l1[1], label=\"ds=\" + o1.data_file_name)\n","    ax[0][1].plot(range(len(l2[1])), l2[1], label=\"ds=\" + o2.data_file_name)\n","    ax[0][1].title.set_text(\"Val Loss | Model Hidden Size = {}\".format(o1.hidden_size))\n","\n","    ax[1][0].plot(range(len(mean_l3)), mean_l3, label=\"ds=\" + o3.data_file_name)\n","    ax[1][0].plot(range(len(mean_l4)), mean_l4, label=\"ds=\" + o4.data_file_name)\n","    ax[1][0].title.set_text(\n","        \"Train Loss | Model Hidden Size = {}\".format(o3.hidden_size)\n","    )\n","\n","    ax[1][1].plot(range(len(l3[1])), l3[1], label=\"ds=\" + o3.data_file_name)\n","    ax[1][1].plot(range(len(l4[1])), l4[1], label=\"ds=\" + o4.data_file_name)\n","    ax[1][1].title.set_text(\"Val Loss | Model Hidden Size = {}\".format(o4.hidden_size))\n","\n","    for i in range(2):\n","        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n","        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n","        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n","        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n","        ax[i][0].legend(loc=\"upper right\")\n","        ax[i][1].legend(loc=\"upper right\")\n","\n","    fig.suptitle(\"Performance by Dataset Size\", fontsize=16)\n","    plt.tight_layout()\n","    fig.subplots_adjust(top=0.9)\n","    plt.legend()\n","    plt.savefig(\"./loss_plot_{}.pdf\".format(fn))\n","    plt.close()\n","\n","\n","def save_loss_comparison_by_hidden(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n","    \"\"\"Plot comparison of training and validation loss curves from all four\n","    runs in Part 3, comparing by hidden size while holding dataset constant.\n","\n","    Models within each pair (l1, l3) and (l2, l4) have the same dataset.\n","\n","    Arguments:\n","        l1: Tuple of lists containing training / val losses for model 1.\n","        l2: Tuple of lists containing training / val losses for model 2.\n","        l3: Tuple of lists containing training / val losses for model 3.\n","        l4: Tuple of lists containing training / val losses for model 4.\n","        o1: Options for model 1.\n","        o2: Options for model 2.\n","        o3: Options for model 3.\n","        o4: Options for model 4.\n","        fn: Output file name.\n","        s: Number of training iterations to average over.\n","    \"\"\"\n","    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n","    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n","    mean_l3 = [np.mean(l3[0][i * s : (i + 1) * s]) for i in range(len(l3[0]) // s)]\n","    mean_l4 = [np.mean(l4[0][i * s : (i + 1) * s]) for i in range(len(l4[0]) // s)]\n","\n","    plt.figure()\n","    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n","\n","    ax[0][0].plot(range(len(mean_l1)), mean_l1, label=\"hid_size=\" + str(o1.hidden_size))\n","    ax[0][0].plot(range(len(mean_l3)), mean_l3, label=\"hid_size=\" + str(o3.hidden_size))\n","    ax[0][0].title.set_text(\"Train Loss | Dataset = \" + o1.data_file_name)\n","\n","    # Validation losses are assumed to be by epoch\n","    ax[0][1].plot(range(len(l1[1])), l1[1], label=\"hid_size=\" + str(o1.hidden_size))\n","    ax[0][1].plot(range(len(l3[1])), l3[1], label=\"hid_size=\" + str(o3.hidden_size))\n","    ax[0][1].title.set_text(\"Val Loss | Dataset = \" + o1.data_file_name)\n","\n","    ax[1][0].plot(range(len(mean_l2)), mean_l2, label=\"hid_size=\" + str(o2.hidden_size))\n","    ax[1][0].plot(range(len(mean_l4)), mean_l4, label=\"hid_size=\" + str(o4.hidden_size))\n","    ax[1][0].title.set_text(\"Train Loss | Dataset = \" + o3.data_file_name)\n","\n","    ax[1][1].plot(range(len(l2[1])), l2[1], label=\"hid_size=\" + str(o2.hidden_size))\n","    ax[1][1].plot(range(len(l4[1])), l4[1], label=\"hid_size=\" + str(o4.hidden_size))\n","    ax[1][1].title.set_text(\"Val Loss | Dataset = \" + o4.data_file_name)\n","\n","    for i in range(2):\n","        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n","        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n","        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n","        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n","        ax[i][0].legend(loc=\"upper right\")\n","        ax[i][1].legend(loc=\"upper right\")\n","\n","    fig.suptitle(\"Performance by Hidden State Size\", fontsize=16)\n","    plt.tight_layout()\n","    fig.subplots_adjust(top=0.9)\n","    plt.legend()\n","    plt.savefig(\"./loss_plot_{}.pdf\".format(fn))\n","    plt.close()\n","\n","\n","def checkpoint(encoder, decoder, idx_dict, opts):\n","    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n","    contains the char_to_index and index_to_char mappings, and the start_token\n","    and end_token values.\n","    \"\"\"\n","    with open(os.path.join(opts.checkpoint_path, \"encoder.pt\"), \"wb\") as f:\n","        torch.save(encoder, f)\n","\n","    with open(os.path.join(opts.checkpoint_path, \"decoder.pt\"), \"wb\") as f:\n","        torch.save(decoder, f)\n","\n","    with open(os.path.join(opts.checkpoint_path, \"idx_dict.pkl\"), \"wb\") as f:\n","        pkl.dump(idx_dict, f)"]},{"cell_type":"markdown","metadata":{"id":"pbvpn4MaV0I1"},"source":["## Data loader"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"XVT4TNTOV3Eg","executionInfo":{"status":"ok","timestamp":1647557193576,"user_tz":240,"elapsed":194,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}}},"outputs":[],"source":["def read_lines(filename):\n","    \"\"\"Read a file and split it into lines.\"\"\"\n","    lines = open(filename).read().strip().lower().split(\"\\n\")\n","    return lines\n","\n","\n","def read_pairs(filename):\n","    \"\"\"Reads lines that consist of two words, separated by a space.\n","\n","    Returns:\n","        source_words: A list of the first word in each line of the file.\n","        target_words: A list of the second word in each line of the file.\n","    \"\"\"\n","    lines = read_lines(filename)\n","    source_words, target_words = [], []\n","    for line in lines:\n","        line = line.strip()\n","        if line:\n","            source, target = line.split()\n","            source_words.append(source)\n","            target_words.append(target)\n","    return source_words, target_words\n","\n","\n","def all_alpha_or_dash(s):\n","    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\"\"\"\n","    return all(c.isalpha() or c == \"-\" for c in s)\n","\n","\n","def filter_lines(lines):\n","    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\"\"\"\n","    return [line for line in lines if all_alpha_or_dash(line)]\n","\n","\n","def load_data(file_name):\n","    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\"\"\"\n","    path = \"./data/{}.txt\".format(file_name)\n","    source_lines, target_lines = read_pairs(path)\n","\n","    # Filter lines\n","    source_lines = filter_lines(source_lines)\n","    target_lines = filter_lines(target_lines)\n","\n","    all_characters = set(\"\".join(source_lines)) | set(\"\".join(target_lines))\n","\n","    # Create a dictionary mapping each character to a unique index\n","    char_to_index = {\n","        char: index for (index, char) in enumerate(sorted(list(all_characters)))\n","    }\n","\n","    # Add start and end tokens to the dictionary\n","    start_token = len(char_to_index)\n","    end_token = len(char_to_index) + 1\n","    char_to_index[\"SOS\"] = start_token\n","    char_to_index[\"EOS\"] = end_token\n","\n","    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n","    index_to_char = {index: char for (char, index) in char_to_index.items()}\n","\n","    # Store the final size of the vocabulary\n","    vocab_size = len(char_to_index)\n","\n","    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n","\n","    idx_dict = {\n","        \"char_to_index\": char_to_index,\n","        \"index_to_char\": index_to_char,\n","        \"start_token\": start_token,\n","        \"end_token\": end_token,\n","    }\n","\n","    return line_pairs, vocab_size, idx_dict\n","\n","\n","def create_dict(pairs):\n","    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n","    This is used to make batches: each batch consists of two parallel tensors, one containing\n","    all source indexes and the other containing all corresponding target indexes.\n","    Within a batch, all the source words are the same length, and all the target words are\n","    the same length.\n","    \"\"\"\n","    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n","\n","    d = defaultdict(list)\n","    for (s, t) in unique_pairs:\n","        d[(len(s), len(t))].append((s, t))\n","\n","    return d"]},{"cell_type":"markdown","metadata":{"id":"bRWfRdmVVjUl"},"source":["## Training and evaluation code"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"wa5-onJhoSeM","executionInfo":{"status":"ok","timestamp":1647557194911,"user_tz":240,"elapsed":1337,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}}},"outputs":[],"source":["def string_to_index_list(s, char_to_index, end_token):\n","    \"\"\"Converts a sentence into a list of indexes (for each character).\"\"\"\n","    return [char_to_index[char] for char in s] + [\n","        end_token\n","    ]  # Adds the end token to each index list\n","\n","\n","def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n","    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n","    words (whitespace-separated), running the encoder-decoder model to translate each\n","    word independently, and then stitching the words back together with spaces between them.\n","    \"\"\"\n","    if idx_dict is None:\n","        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n","    return \" \".join(\n","        [translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()]\n","    )\n","\n","\n","def translate(input_string, encoder, decoder, idx_dict, opts):\n","    \"\"\"Translates a given string from English to Pig-Latin.\"\"\"\n","\n","    char_to_index = idx_dict[\"char_to_index\"]\n","    index_to_char = idx_dict[\"index_to_char\"]\n","    start_token = idx_dict[\"start_token\"]\n","    end_token = idx_dict[\"end_token\"]\n","\n","    max_generated_chars = 20\n","    gen_string = \"\"\n","\n","    indexes = string_to_index_list(input_string, char_to_index, end_token)\n","    indexes = to_var(\n","        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n","    )  # Unsqueeze to make it like BS = 1\n","\n","    encoder_annotations, encoder_last_hidden = encoder(indexes)\n","\n","    decoder_hidden = encoder_last_hidden\n","    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n","    decoder_inputs = decoder_input\n","\n","    for i in range(max_generated_chars):\n","        ## slow decoding, recompute everything at each time\n","        decoder_outputs, attention_weights = decoder(\n","            decoder_inputs, encoder_annotations, decoder_hidden\n","        )\n","\n","        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n","        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n","        ni = ni[-1]  # latest output token\n","\n","        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n","\n","        if ni == end_token:\n","            break\n","        else:\n","            gen_string = \"\".join(\n","                [\n","                    index_to_char[int(item)]\n","                    for item in generated_words.cpu().numpy().reshape(-1)\n","                ]\n","            )\n","\n","    return gen_string\n","\n","\n","def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n","    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\"\"\"\n","    if idx_dict is None:\n","        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n","    char_to_index = idx_dict[\"char_to_index\"]\n","    index_to_char = idx_dict[\"index_to_char\"]\n","    start_token = idx_dict[\"start_token\"]\n","    end_token = idx_dict[\"end_token\"]\n","\n","    max_generated_chars = 20\n","    gen_string = \"\"\n","\n","    indexes = string_to_index_list(input_string, char_to_index, end_token)\n","    indexes = to_var(\n","        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n","    )  # Unsqueeze to make it like BS = 1\n","\n","    encoder_annotations, encoder_hidden = encoder(indexes)\n","\n","    decoder_hidden = encoder_hidden\n","    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n","    decoder_inputs = decoder_input\n","\n","    produced_end_token = False\n","\n","    for i in range(max_generated_chars):\n","        ## slow decoding, recompute everything at each time\n","        decoder_outputs, attention_weights = decoder(\n","            decoder_inputs, encoder_annotations, decoder_hidden\n","        )\n","        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n","        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n","        ni = ni[-1]  # latest output token\n","\n","        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n","\n","        if ni == end_token:\n","            break\n","        else:\n","            gen_string = \"\".join(\n","                [\n","                    index_to_char[int(item)]\n","                    for item in generated_words.cpu().numpy().reshape(-1)\n","                ]\n","            )\n","\n","    if isinstance(attention_weights, tuple):\n","        ## transformer's attention mweights\n","        attention_weights, self_attention_weights = attention_weights\n","\n","    all_attention_weights = attention_weights.data.cpu().numpy()\n","\n","    for i in range(len(all_attention_weights)):\n","        attention_weights_matrix = all_attention_weights[i].squeeze()\n","        fig = plt.figure()\n","        ax = fig.add_subplot(111)\n","        cax = ax.matshow(attention_weights_matrix, cmap=\"bone\")\n","        fig.colorbar(cax)\n","\n","        # Set up axes\n","        ax.set_yticklabels([\"\"] + list(input_string) + [\"EOS\"], rotation=90)\n","        ax.set_xticklabels(\n","            [\"\"] + list(gen_string) + ([\"EOS\"] if produced_end_token else [])\n","        )\n","\n","        # Show label at every tick\n","        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","        # Add title\n","        plt.xlabel(\"Attention weights to the source sentence in layer {}\".format(i + 1))\n","        plt.tight_layout()\n","        plt.grid(\"off\")\n","        plt.show()\n","\n","    return gen_string\n","\n","\n","def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n","    \"\"\"Train/Evaluate the model on a dataset.\n","\n","    Arguments:\n","        data_dict: The validation/test word pairs, organized by source and target lengths.\n","        encoder: An encoder model to produce annotations for each step of the input sequence.\n","        decoder: A decoder model (with or without attention) to generate output tokens.\n","        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n","        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n","        optimizer: Train the weights if an optimizer is given. None if only evaluate the model.\n","        opts: The command-line arguments.\n","\n","    Returns:\n","        mean_loss: The average loss over all batches from data_dict.\n","    \"\"\"\n","    start_token = idx_dict[\"start_token\"]\n","    end_token = idx_dict[\"end_token\"]\n","    char_to_index = idx_dict[\"char_to_index\"]\n","\n","    losses = []\n","    for key in data_dict:\n","        input_strings, target_strings = zip(*data_dict[key])\n","        input_tensors = [\n","            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n","            for s in input_strings\n","        ]\n","        target_tensors = [\n","            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n","            for s in target_strings\n","        ]\n","\n","        num_tensors = len(input_tensors)\n","        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n","\n","        for i in range(num_batches):\n","\n","            start = i * opts.batch_size\n","            end = start + opts.batch_size\n","\n","            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n","            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n","\n","            # The batch size may be different in each epoch\n","            BS = inputs.size(0)\n","\n","            encoder_annotations, encoder_hidden = encoder(inputs)\n","\n","            # The last hidden state of the encoder becomes the first hidden state of the decoder\n","            decoder_hidden = encoder_hidden\n","\n","            start_vector = (\n","                torch.ones(BS).long().unsqueeze(1) * start_token\n","            )  # BS x 1 --> 16x1  CHECKED\n","            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n","\n","            loss = 0.0\n","\n","            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n","\n","            decoder_inputs = torch.cat(\n","                [decoder_input, targets[:, 0:-1]], dim=1\n","            )  # Gets decoder inputs by shifting the targets to the right\n","\n","            decoder_outputs, attention_weights = decoder(\n","                decoder_inputs, encoder_annotations, decoder_hidden\n","            )\n","            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n","            targets_flatten = targets.view(-1)\n","\n","            loss = criterion(decoder_outputs_flatten, targets_flatten)\n","\n","            losses.append(loss.item())\n","\n","            ## training if an optimizer is provided\n","            if optimizer:\n","                # Zero gradients\n","                optimizer.zero_grad()\n","                # Compute gradients\n","                loss.backward()\n","                # Update the parameters of the encoder and decoder\n","                optimizer.step()\n","\n","    return losses\n","\n","\n","def training_loop(\n","    train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n","):\n","    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n","        * Prints training and val loss each epoch.\n","        * Prints qualitative translation results each epoch using TEST_SENTENCE\n","        * Saves an attention map for TEST_WORD_ATTN each epoch\n","        * Returns loss curves for comparison\n","\n","    Arguments:\n","        train_dict: The training word pairs, organized by source and target lengths.\n","        val_dict: The validation word pairs, organized by source and target lengths.\n","        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n","        encoder: An encoder model to produce annotations for each step of the input sequence.\n","        decoder: A decoder model (with or without attention) to generate output tokens.\n","        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n","        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n","        opts: The command-line arguments.\n","\n","    Returns:\n","        losses: Lists containing training and validation loss curves.\n","    \"\"\"\n","\n","    start_token = idx_dict[\"start_token\"]\n","    end_token = idx_dict[\"end_token\"]\n","    char_to_index = idx_dict[\"char_to_index\"]\n","\n","    loss_log = open(os.path.join(opts.checkpoint_path, \"loss_log.txt\"), \"w\")\n","\n","    best_val_loss = 1e6\n","    train_losses = []\n","    val_losses = []\n","\n","    mean_train_losses = []\n","    mean_val_losses = []\n","\n","    early_stopping_counter = 0\n","\n","    for epoch in range(opts.nepochs):\n","\n","        optimizer.param_groups[0][\"lr\"] *= opts.lr_decay\n","\n","        train_loss = compute_loss(\n","            train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts\n","        )\n","        val_loss = compute_loss(\n","            val_dict, encoder, decoder, idx_dict, criterion, None, opts\n","        )\n","\n","        mean_train_loss = np.mean(train_loss)\n","        mean_val_loss = np.mean(val_loss)\n","\n","        if mean_val_loss < best_val_loss:\n","            checkpoint(encoder, decoder, idx_dict, opts)\n","            best_val_loss = mean_val_loss\n","            early_stopping_counter = 0\n","        else:\n","            early_stopping_counter += 1\n","\n","        if early_stopping_counter > opts.early_stopping_patience:\n","            print(\n","                \"Validation loss has not improved in {} epochs, stopping early\".format(\n","                    opts.early_stopping_patience\n","                )\n","            )\n","            print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n","            return (train_losses, mean_val_losses)\n","\n","        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n","        print(\n","            \"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(\n","                epoch, mean_train_loss, mean_val_loss, gen_string\n","            )\n","        )\n","\n","        loss_log.write(\"{} {} {}\\n\".format(epoch, train_loss, val_loss))\n","        loss_log.flush()\n","\n","        train_losses += train_loss\n","        val_losses += val_loss\n","\n","        mean_train_losses.append(mean_train_loss)\n","        mean_val_losses.append(mean_val_loss)\n","\n","        save_loss_plot(mean_train_losses, mean_val_losses, opts)\n","\n","    print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n","    return (train_losses, mean_val_losses)\n","\n","\n","def print_data_stats(line_pairs, vocab_size, idx_dict):\n","    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\"\"\"\n","    print(\"=\" * 80)\n","    print(\"Data Stats\".center(80))\n","    print(\"-\" * 80)\n","    for pair in line_pairs[:5]:\n","        print(pair)\n","    print(\"Num unique word pairs: {}\".format(len(line_pairs)))\n","    print(\"Vocabulary: {}\".format(idx_dict[\"char_to_index\"].keys()))\n","    print(\"Vocab size: {}\".format(vocab_size))\n","    print(\"=\" * 80)\n","\n","\n","def train(opts):\n","    line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n","    print_data_stats(line_pairs, vocab_size, idx_dict)\n","\n","    # Split the line pairs into an 80% train and 20% val split\n","    num_lines = len(line_pairs)\n","    num_train = int(0.8 * num_lines)\n","    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n","\n","    # Group the data by the lengths of the source and target words, to form batches\n","    train_dict = create_dict(train_pairs)\n","    val_dict = create_dict(val_pairs)\n","\n","    ##########################################################################\n","    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n","    ##########################################################################\n","    if opts.encoder_type == \"rnn\":\n","        encoder = GRUEncoder(\n","            vocab_size=vocab_size, hidden_size=opts.hidden_size, opts=opts\n","        )\n","    elif opts.encoder_type == \"transformer\":\n","        encoder = TransformerEncoder(\n","            vocab_size=vocab_size,\n","            hidden_size=opts.hidden_size,\n","            num_layers=opts.num_transformer_layers,\n","            opts=opts,\n","        )\n","    elif opts.encoder_type == \"attention\":\n","      encoder = AttentionEncoder(\n","            vocab_size=vocab_size,\n","            hidden_size=opts.hidden_size,\n","            opts=opts,\n","        )\n","    else:\n","        raise NotImplementedError\n","\n","    if opts.decoder_type == \"rnn\":\n","        decoder = RNNDecoder(vocab_size=vocab_size, hidden_size=opts.hidden_size)\n","    elif opts.decoder_type == \"rnn_attention\":\n","        decoder = RNNAttentionDecoder(\n","            vocab_size=vocab_size,\n","            hidden_size=opts.hidden_size,\n","            attention_type=opts.attention_type,\n","        )\n","    elif opts.decoder_type == \"transformer\":\n","        decoder = TransformerDecoder(\n","            vocab_size=vocab_size,\n","            hidden_size=opts.hidden_size,\n","            num_layers=opts.num_transformer_layers,\n","        )\n","    elif opts.encoder_type == \"attention\":\n","      decoder = AttentionDecoder(\n","            vocab_size=vocab_size,\n","            hidden_size=opts.hidden_size,\n","        )\n","    else:\n","        raise NotImplementedError\n","\n","    #### setup checkpoint path\n","    model_name = \"h{}-bs{}-{}-{}\".format(\n","        opts.hidden_size, opts.batch_size, opts.decoder_type, opts.data_file_name\n","    )\n","    opts.checkpoint_path = model_name\n","    create_dir_if_not_exists(opts.checkpoint_path)\n","    ####\n","\n","    if opts.cuda:\n","        encoder.cuda()\n","        decoder.cuda()\n","        print(\"Moved models to GPU!\")\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(\n","        list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate\n","    )\n","\n","    try:\n","        losses = training_loop(\n","            train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n","        )\n","    except KeyboardInterrupt:\n","        print(\"Exiting early from training.\")\n","        return encoder, decoder, losses\n","\n","    return encoder, decoder, losses\n","\n","\n","def print_opts(opts):\n","    \"\"\"Prints the values of all command-line arguments.\"\"\"\n","    print(\"=\" * 80)\n","    print(\"Opts\".center(80))\n","    print(\"-\" * 80)\n","    for key in opts.__dict__:\n","        print(\"{:>30}: {:<30}\".format(key, opts.__dict__[key]).center(80))\n","    print(\"=\" * 80)"]},{"cell_type":"markdown","metadata":{"id":"0yh08KhgnA30"},"source":["## Download dataset"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"aROU2xZanDKq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647557195264,"user_tz":240,"elapsed":356,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}},"outputId":"6ee8b144-9ece-4640-b2bc-e43b17e39458"},"outputs":[{"output_type":"stream","name":"stdout","text":["data/pig_latin_small.txt\n","Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_small.txt\n","data/pig_latin_large.txt\n","Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_large.txt\n"]}],"source":["######################################################################\n","# Download Translation datasets\n","######################################################################\n","data_fpath = get_file(\n","    fname=\"pig_latin_small.txt\",\n","    origin=\"http://www.cs.toronto.edu/~jba/pig_latin_small.txt\",\n","    untar=False,\n",")\n","\n","data_fpath = get_file(\n","    fname=\"pig_latin_large.txt\",\n","    origin=\"http://www.cs.toronto.edu/~jba/pig_latin_large.txt\",\n","    untar=False,\n",")"]},{"cell_type":"markdown","metadata":{"id":"YDYMr7NclZdw"},"source":["# Part 1: Neural machine translation (NMT)\n","\n","In this section, you will implement a Gated Recurrent Unit (GRU) cell, a common type of recurrent neural network (RNN). The GRU cell is a simplification of the Long Short-Term Memory cell. Therefore, we have provided you with an implemented LSTM cell (`MyLSTMCell`), which you can reference when completing `MyGRUCell`."]},{"cell_type":"code","execution_count":19,"metadata":{"id":"cOnALRQkkjDO","executionInfo":{"status":"ok","timestamp":1647557196888,"user_tz":240,"elapsed":74,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}}},"outputs":[],"source":["class MyLSTMCell(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(MyLSTMCell, self).__init__()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","\n","        self.Wif = nn.Linear(input_size, hidden_size)\n","        self.Whf = nn.Linear(hidden_size, hidden_size)\n","\n","        self.Wii = nn.Linear(input_size, hidden_size)\n","        self.Whi = nn.Linear(hidden_size, hidden_size)\n","\n","        self.Wic = nn.Linear(input_size, hidden_size)\n","        self.Whc = nn.Linear(hidden_size, hidden_size)\n","\n","        self.Wio = nn.Linear(input_size, hidden_size)\n","        self.Who = nn.Linear(hidden_size, hidden_size)\n","\n","    def forward(self, x, h_prev, c_prev):\n","        \"\"\"Forward pass of the LSTM computation for one time step.\n","\n","        Arguments\n","            x: batch_size x input_size\n","            h_prev: batch_size x hidden_size\n","            c_prev: batch_size x hidden_size\n","\n","        Returns:\n","            h_new: batch_size x hidden_size\n","            c_new: batch_size x hidden_size\n","        \"\"\"\n","\n","        f = torch.sigmoid(self.Wif(x) + self.Whf(h_prev))\n","        i = torch.sigmoid(self.Wii(x) + self.Whi(h_prev))\n","\n","        c = torch.tanh(self.Wic(x) + self.Whc(h_prev))\n","        o = torch.sigmoid(self.Wio(x) + self.Who(h_prev))\n","\n","        c_new = f * c_prev + i * c\n","        h_new = o * torch.tanh(c_new)\n","\n","        return h_new, c_new"]},{"cell_type":"markdown","metadata":{"id":"dCae1mOUlZrC"},"source":["## Step 1: GRU Cell\n","Please implement the `MyGRUCell` class defined in the next cell. "]},{"cell_type":"code","execution_count":20,"metadata":{"id":"DGyxqZIQzTJH","executionInfo":{"status":"ok","timestamp":1647557206709,"user_tz":240,"elapsed":85,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}}},"outputs":[],"source":["class MyGRUCell(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(MyGRUCell, self).__init__()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        # Input linear layers\n","        self.Wiz = nn.Linear(input_size, hidden_size)\n","        self.Wir = nn.Linear(input_size, hidden_size)\n","        self.Wih = nn.Linear(input_size, hidden_size)\n","\n","        # Hidden linear layers\n","        self.Whz = nn.Linear(hidden_size, hidden_size)\n","        self.Whr = nn.Linear(hidden_size, hidden_size)\n","        self.Whh = nn.Linear(hidden_size, hidden_size)\n","\n","    def forward(self, x, h_prev):\n","        \"\"\"Forward pass of the GRU computation for one time step.\n","\n","        Arguments\n","            x: batch_size x input_size\n","            h_prev: batch_size x hidden_size\n","\n","        Returns:\n","            h_new: batch_size x hidden_size\n","        \"\"\"\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        z = torch.sigmoid(self.Wiz(x) + self.Whz(h_prev))\n","        r = torch.sigmoid(self.Wir(x) + self.Whr(h_prev))\n","        g = torch.tanh(self.Wih(x) + (self.Whh(r * h_prev)))\n","        h_new = (1-z) * h_prev + z * g\n","        return h_new"]},{"cell_type":"markdown","metadata":{"id":"ecEq4TP2lZ4Z"},"source":["## Step 2: GRU Encoder\n","\n","The following cells use your `MyGRUCell` implementation to build a recurrent encoder and decoder. Please read the implementations to understand what they do and run the cells before proceeding."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"8jDNim2fmVJV","executionInfo":{"status":"ok","timestamp":1647557208132,"user_tz":240,"elapsed":88,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}}},"outputs":[],"source":["class GRUEncoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, opts):\n","        super(GRUEncoder, self).__init__()\n","\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.opts = opts\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.gru = MyGRUCell(hidden_size, hidden_size)\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward pass of the encoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n","\n","        Returns:\n","            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n","        \"\"\"\n","\n","        batch_size, seq_len = inputs.size()\n","        hidden = self.init_hidden(batch_size)\n","\n","        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","        annotations = []\n","\n","        for i in range(seq_len):\n","            x = encoded[:, i, :]  # Get the current time step, across the whole batch\n","            hidden = self.gru(x, hidden)\n","            annotations.append(hidden)\n","\n","        annotations = torch.stack(annotations, dim=1)\n","        return annotations, hidden\n","\n","    def init_hidden(self, bs):\n","        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n","        of a batch of sequences.\n","\n","        Arguments:\n","            bs: The batch size for the initial hidden state.\n","\n","        Returns:\n","            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n","        \"\"\"\n","        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"HvwizYM9ma4p","executionInfo":{"status":"ok","timestamp":1647557208220,"user_tz":240,"elapsed":2,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}}},"outputs":[],"source":["class RNNDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size):\n","        super(RNNDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.rnn = MyGRUCell(input_size=hidden_size, hidden_size=hidden_size)\n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, inputs, annotations, hidden_init):\n","        \"\"\"Forward pass of the non-attentional decoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch. (batch_size x seq_len)\n","            annotations: This is not used here. It just maintains consistency with the\n","                    interface used by the AttentionDecoder class.\n","            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n","\n","        Returns:\n","            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n","            None\n","        \"\"\"\n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","\n","        hiddens = []\n","        h_prev = hidden_init\n","\n","        for i in range(seq_len):\n","            x = embed[\n","                :, i, :\n","            ]  # Get the current time step input tokens, across the whole batch\n","            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n","            hiddens.append(h_prev)\n","\n","        hiddens = torch.stack(hiddens, dim=1)  # batch_size x seq_len x hidden_size\n","\n","        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n","        return output, None"]},{"cell_type":"markdown","metadata":{"id":"TSDTbsydlaGI"},"source":["## Step 3: Training and Analysis\n","\n","Train the encoder-decoder model to perform English --> Pig Latin translation. We will start by training on the smaller dataset."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"XmVuXTozTPF7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647557458489,"user_tz":240,"elapsed":249270,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}},"outputId":"1ff6df76-3a54-4fbc-f8da-eb142487689a"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_small                        \n","                                   cuda: 1                                      \n","                                nepochs: 50                                     \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.005                                  \n","                               lr_decay: 0.99                                   \n","                early_stopping_patience: 20                                     \n","                             batch_size: 64                                     \n","                            hidden_size: 32                                     \n","                           encoder_type: rnn                                    \n","                           decoder_type: rnn                                    \n","                         attention_type:                                        \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('tricks', 'ickstray')\n","('trembled', 'embledtray')\n","('novelty', 'oveltynay')\n","('quiet', 'ietquay')\n","('played', 'ayedplay')\n","Num unique word pairs: 3198\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 2.320 | Val loss: 2.047 | Gen: ay ay illay illay illay\n","Epoch:   1 | Train loss: 1.859 | Val loss: 1.866 | Gen: ay-oray ay-atingway ontay illay ontay\n","Epoch:   2 | Train loss: 1.684 | Val loss: 1.740 | Gen: ay-ortay ay-ingway ontay-oway illay-ortay oray\n","Epoch:   3 | Train loss: 1.551 | Val loss: 1.682 | Gen: eray ay-ouray ontay ilway-omay orsay\n","Epoch:   4 | Train loss: 1.440 | Val loss: 1.591 | Gen: eray ailway ontionday issay-omway ortiongway\n","Epoch:   5 | Train loss: 1.337 | Val loss: 1.561 | Gen: eray ay-iedeway ontionday issay orroutingway\n","Epoch:   6 | Train loss: 1.254 | Val loss: 1.532 | Gen: eway aicelay ontionday issay-otionday ortiongay\n","Epoch:   7 | Train loss: 1.215 | Val loss: 1.489 | Gen: eway arday oncilway issay ortiongway\n","Epoch:   8 | Train loss: 1.145 | Val loss: 1.471 | Gen: ehay ay-iecehay ontingway isway ortionday\n","Epoch:   9 | Train loss: 1.106 | Val loss: 1.378 | Gen: ehay aicerway ontionday isway orktay\n","Epoch:  10 | Train loss: 1.038 | Val loss: 1.386 | Gen: ehay aingray oncilway-oday isway orktay\n","Epoch:  11 | Train loss: 0.997 | Val loss: 1.364 | Gen: ehay aingray oncilway isway orktay\n","Epoch:  12 | Train loss: 0.954 | Val loss: 1.337 | Gen: ehay aidray oncilway-oday isway orkingway\n","Epoch:  13 | Train loss: 0.926 | Val loss: 1.321 | Gen: ethay aidray oncilway-oday isway orkway-oreday\n","Epoch:  14 | Train loss: 0.888 | Val loss: 1.325 | Gen: ehay aidray oncifay-oday isway orkway-awlay\n","Epoch:  15 | Train loss: 0.859 | Val loss: 1.329 | Gen: ethay aidray ontionday-oday-oday isway orkway-away-etetay\n","Epoch:  16 | Train loss: 0.832 | Val loss: 1.286 | Gen: ethay aidray ontionday isway orkway\n","Epoch:  17 | Train loss: 0.826 | Val loss: 1.303 | Gen: ethay ay-iendray oncionway isway orkway-abletay\n","Epoch:  18 | Train loss: 0.828 | Val loss: 1.310 | Gen: ethay ay-ieceredway ontionday-oday isway orkway\n","Epoch:  19 | Train loss: 0.797 | Val loss: 1.325 | Gen: ehay aidray ontionday isway orkway\n","Epoch:  20 | Train loss: 0.774 | Val loss: 1.279 | Gen: ethay ay-ingray ontionday-oday isway orkway-ardway\n","Epoch:  21 | Train loss: 0.744 | Val loss: 1.336 | Gen: ethay aidray ontionday-oday isway otkingway\n","Epoch:  22 | Train loss: 0.726 | Val loss: 1.259 | Gen: ethay aidray ontionday-oday-oday isway orkway-ardway\n","Epoch:  23 | Train loss: 0.707 | Val loss: 1.343 | Gen: ethay aidray ontinglay-awlay isway orkway-awlay\n","Epoch:  24 | Train loss: 0.697 | Val loss: 1.304 | Gen: ethay aidbray ontionday-oday-oday isway otkeningnay\n","Epoch:  25 | Train loss: 0.672 | Val loss: 1.287 | Gen: ethay aidray ontionday-oday isway orkway\n","Epoch:  26 | Train loss: 0.667 | Val loss: 1.298 | Gen: etay aidbray ontinglay-awlay isway orkway-agnetray\n","Epoch:  27 | Train loss: 0.657 | Val loss: 1.276 | Gen: ethay aidray ontioncilationway isway okingnay\n","Epoch:  28 | Train loss: 0.641 | Val loss: 1.293 | Gen: ethay aidbray ontioncilationway isway orkway-agnay\n","Epoch:  29 | Train loss: 0.618 | Val loss: 1.240 | Gen: ethay aidray ontingcay-inway-awla isway orkway\n","Epoch:  30 | Train loss: 0.606 | Val loss: 1.361 | Gen: ethay aidbray ontingcay-awlay isway orkway-agnay\n","Epoch:  31 | Train loss: 0.647 | Val loss: 1.289 | Gen: ethay ainderway ontionday-oday isway orkway\n","Epoch:  32 | Train loss: 0.638 | Val loss: 1.319 | Gen: ethay aidray ontiondingtay-oday isway orkway-agdray\n","Epoch:  33 | Train loss: 0.602 | Val loss: 1.235 | Gen: ethay aidbray ontioncilationway isway okenchay\n","Epoch:  34 | Train loss: 0.569 | Val loss: 1.270 | Gen: ethay aidbray ontiondingway isway orkway-ablengay\n","Epoch:  35 | Train loss: 0.557 | Val loss: 1.255 | Gen: ethay aidray ontinglay-oday isway orkenway\n","Epoch:  36 | Train loss: 0.540 | Val loss: 1.292 | Gen: ethay aidray ontioncingway isway otkingdray\n","Epoch:  37 | Train loss: 0.534 | Val loss: 1.287 | Gen: ethay aidray ontioncilageway isway orkingdray\n","Epoch:  38 | Train loss: 0.527 | Val loss: 1.293 | Gen: ethay aidbray-inghay ontioncingway isway orkway-ardbay\n","Epoch:  39 | Train loss: 0.525 | Val loss: 1.262 | Gen: ethay aidbray oncioncingway isway oktygray\n","Epoch:  40 | Train loss: 0.524 | Val loss: 1.294 | Gen: ethay aidbray ontingcay-awlay isway orkway-abletay\n","Epoch:  41 | Train loss: 0.508 | Val loss: 1.314 | Gen: ethay aidbray ontingcay isway okenwray\n","Epoch:  42 | Train loss: 0.501 | Val loss: 1.334 | Gen: ethay aidbray ontingcay-itingway isway orkway-abletray\n","Epoch:  43 | Train loss: 0.510 | Val loss: 1.325 | Gen: ethay aidbray ontioncilingway ishay orkway-agneray\n","Epoch:  44 | Train loss: 0.527 | Val loss: 1.319 | Gen: ethay aidray onciontionway isway orkway-abletray\n","Epoch:  45 | Train loss: 0.493 | Val loss: 1.323 | Gen: ethay aidbray ontioncilyway isway oktringgray\n","Epoch:  46 | Train loss: 0.487 | Val loss: 1.270 | Gen: ethay aidray ontioncilay-aceplay isway orkway\n","Epoch:  47 | Train loss: 0.473 | Val loss: 1.305 | Gen: ethay aidray ontioncilay isway orkway\n","Epoch:  48 | Train loss: 0.463 | Val loss: 1.296 | Gen: ethay aidray ontioncilyway isway orkway-abletray\n","Epoch:  49 | Train loss: 0.458 | Val loss: 1.305 | Gen: ethay aidray ontioncingway isway orkway-abletray\n","Obtained lowest validation loss of: 1.2354865216396071\n","source:\t\tthe air conditioning is working \n","translated:\tethay aidray ontioncingway isway orkway-abletray\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","rnn_args_s = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_small\",\n","    \"cuda\": True,\n","    \"nepochs\": 50,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 0.005,\n","    \"lr_decay\": 0.99,\n","    \"early_stopping_patience\": 20,\n","    \"batch_size\": 64,\n","    \"hidden_size\": 32,\n","    \"encoder_type\": \"rnn\",  # options: rnn / transformer\n","    \"decoder_type\": \"rnn\",  # options: rnn / rnn_attention / transformer\n","    \"attention_type\": \"\",   # options: additive / scaled_dot\n","}\n","rnn_args_s.update(args_dict)\n","\n","print_opts(rnn_args_s)\n","rnn_encode_s, rnn_decoder_s, rnn_losses_s = train(rnn_args_s)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, rnn_encode_s, rnn_decoder_s, None, rnn_args_s\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"markdown","metadata":{"id":"0mR97V_NtER6"},"source":["Next, we train on the larger dataset. This experiment investigates if increasing dataset size improves model generalization on the validation set. \n","\n","For a fair comparison, the number of iterations (not number of epochs) for each run should be similar. This is done in a quick and dirty way by adjusting the batch size so approximately the same number of batches is processed per epoch."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"H3YLrAjsmx_W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647557757912,"user_tz":240,"elapsed":299431,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}},"outputId":"929d9238-216d-419b-f069-d35336ace93b"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_large                        \n","                                   cuda: 1                                      \n","                                nepochs: 50                                     \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.005                                  \n","                               lr_decay: 0.99                                   \n","                early_stopping_patience: 10                                     \n","                             batch_size: 512                                    \n","                            hidden_size: 32                                     \n","                           encoder_type: rnn                                    \n","                           decoder_type: rnn                                    \n","                         attention_type:                                        \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('merlin', 'erlinmay')\n","('lq', 'lqay')\n","('fiddle', 'iddlefay')\n","('applied', 'appliedway')\n","('multicast', 'ulticastmay')\n","Num unique word pairs: 22402\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 2.302 | Val loss: 2.021 | Gen: ay ay onsay onsay onsay\n","Epoch:   1 | Train loss: 1.892 | Val loss: 1.874 | Gen: esay antay ontay ingray ontay\n","Epoch:   2 | Train loss: 1.732 | Val loss: 1.794 | Gen: eday anday onsay insay onsay\n","Epoch:   3 | Train loss: 1.624 | Val loss: 1.744 | Gen: eday ay-ingray-andway onsay-inway istay onsay-inway\n","Epoch:   4 | Train loss: 1.524 | Val loss: 1.685 | Gen: eway ay-inway onsay-inway issay onsay-inway\n","Epoch:   5 | Train loss: 1.438 | Val loss: 1.640 | Gen: eway andedway-awlay-awlay onsay-inway issay-awlay onsay-inway\n","Epoch:   6 | Train loss: 1.357 | Val loss: 1.597 | Gen: eway aingray-awlay onsay-inway issay-awlay onsay-inway\n","Epoch:   7 | Train loss: 1.298 | Val loss: 1.565 | Gen: eway aingray-awlay-awlay onssay-inway-awlay issay-inway oodgay-inway-awlay\n","Epoch:   8 | Train loss: 1.254 | Val loss: 1.527 | Gen: eway away-inway onistionsay issay oodgay-inway\n","Epoch:   9 | Train loss: 1.230 | Val loss: 1.513 | Gen: eway aintardway-oday onsay-inway istionsay oodgay-inway\n","Epoch:  10 | Train loss: 1.172 | Val loss: 1.461 | Gen: eway aingray-awlay-awlay onsay-inway istay oodgay-inway\n","Epoch:  11 | Train loss: 1.119 | Val loss: 1.489 | Gen: eway aingray-awlay-awlay onsay-inway istay oodgay-inway\n","Epoch:  12 | Train loss: 1.086 | Val loss: 1.441 | Gen: eway ainteray-inway-awlay oncisticonway issay oodgay-inway-awlay\n","Epoch:  13 | Train loss: 1.038 | Val loss: 1.408 | Gen: eway aingray-ortationsay onictionsay istay orthoushay-inway-awl\n","Epoch:  14 | Train loss: 1.016 | Val loss: 1.429 | Gen: eway aingray-ayday ontay-inway-awlay issay oodgay-inway-awlay\n","Epoch:  15 | Train loss: 1.015 | Val loss: 1.451 | Gen: eway aingray-awlay onntoussictingway istay oodgay-inway\n","Epoch:  16 | Train loss: 0.984 | Val loss: 1.347 | Gen: eway aingray-inway-awlay onticktay-oday istay oodgay-inway-awlay\n","Epoch:  17 | Train loss: 0.938 | Val loss: 1.330 | Gen: eway airay-inway-awlay onconisticay istay ordicturay\n","Epoch:  18 | Train loss: 0.921 | Val loss: 1.351 | Gen: eway airedway-orfay-ofway onictiontay isticationsway ordishmay\n","Epoch:  19 | Train loss: 0.911 | Val loss: 1.359 | Gen: eway airay-inway-awlay onnoustictionway istay ontictionsway\n","Epoch:  20 | Train loss: 0.885 | Val loss: 1.316 | Gen: eway airatierstay onntictiontay istay ountsray-ybay\n","Epoch:  21 | Train loss: 0.868 | Val loss: 1.314 | Gen: etway airationsay onictionsay istay ordicktay\n","Epoch:  22 | Train loss: 0.855 | Val loss: 1.339 | Gen: eway airgray-ayday ontingictionsway issway onssictionsway\n","Epoch:  23 | Train loss: 0.842 | Val loss: 1.302 | Gen: eway airay-ayday onintictionsway isway ordictionsway\n","Epoch:  24 | Train loss: 0.834 | Val loss: 1.410 | Gen: etway airay-inway-awlay oncictictioncay issay oncomplingway\n","Epoch:  25 | Train loss: 0.853 | Val loss: 1.294 | Gen: etway airatierstay onconishionway istay orday-inway-awlay\n","Epoch:  26 | Train loss: 0.805 | Val loss: 1.273 | Gen: etway airay-eway onnticitionsway istionsway orkay-inway-awlay\n","Epoch:  27 | Train loss: 0.783 | Val loss: 1.259 | Gen: eway airatierstay ontingray isway ordictusway\n","Epoch:  28 | Train loss: 0.769 | Val loss: 1.238 | Gen: etway airay-ayday onntictionstay istway orkingway\n","Epoch:  29 | Train loss: 0.759 | Val loss: 1.271 | Gen: etway airay-inway-awlay onitintyway istay ordictway\n","Epoch:  30 | Train loss: 0.759 | Val loss: 1.356 | Gen: eway airay-inway-awlay onsicicway issway orktay-inway-awlay\n","Epoch:  31 | Train loss: 0.763 | Val loss: 1.294 | Gen: etway airgingway onitingway istay orktay-ortedway\n","Epoch:  32 | Train loss: 0.756 | Val loss: 1.310 | Gen: eway airgray-ortay onitinglyway istionsway orikthay\n","Epoch:  33 | Train loss: 0.759 | Val loss: 1.291 | Gen: etway airgray-ayday ontingncycay istway ordhictway\n","Epoch:  34 | Train loss: 0.736 | Val loss: 1.254 | Gen: eway airgray-inway onitingway isway ordhictway\n","Epoch:  35 | Train loss: 0.704 | Val loss: 1.231 | Gen: etway airway-ybay ontingnay isway orkitingway\n","Epoch:  36 | Train loss: 0.688 | Val loss: 1.230 | Gen: etway airgray ontingway isway orkitway\n","Epoch:  37 | Train loss: 0.673 | Val loss: 1.212 | Gen: etway airgray onitingway istway orkingway\n","Epoch:  38 | Train loss: 0.661 | Val loss: 1.195 | Gen: etway airway-ayday onitingway istway orkitway\n","Epoch:  39 | Train loss: 0.650 | Val loss: 1.224 | Gen: etway airway-ybay onnitictionsway isway orkingway\n","Epoch:  40 | Train loss: 0.651 | Val loss: 1.236 | Gen: ethay airway-ayday onitictionstay istway orkitway\n","Epoch:  41 | Train loss: 0.677 | Val loss: 1.252 | Gen: etway airway-ybay ontingnay istway orkingway\n","Epoch:  42 | Train loss: 0.697 | Val loss: 1.400 | Gen: ethay airgroway ontingctyway istway oudblfay-ortodingway\n","Epoch:  43 | Train loss: 0.756 | Val loss: 1.352 | Gen: etway amirgay ontingway istway oudway-ingsay\n","Epoch:  44 | Train loss: 0.719 | Val loss: 1.334 | Gen: ethay airmay-inway-ayday ontingway istway ordway-ightnay\n","Epoch:  45 | Train loss: 0.682 | Val loss: 1.277 | Gen: etway airway-inway onintingway istay orthingway\n","Epoch:  46 | Train loss: 0.649 | Val loss: 1.246 | Gen: ethay airgray ontingictionsway isway orkingway\n","Epoch:  47 | Train loss: 0.629 | Val loss: 1.198 | Gen: ethay airway-ybay ontingnay istway orkingway\n","Epoch:  48 | Train loss: 0.605 | Val loss: 1.169 | Gen: ethay airway-inway-ayday ontingnay isway orkingway\n","Epoch:  49 | Train loss: 0.592 | Val loss: 1.162 | Gen: etway airway-ybay onitingway isway orkingway\n","Obtained lowest validation loss of: 1.1621543825245821\n","source:\t\tthe air conditioning is working \n","translated:\tetway airway-ybay onitingway isway orkingway\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","rnn_args_l = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_large\",\n","    \"cuda\": True,\n","    \"nepochs\": 50,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 0.005,\n","    \"lr_decay\": 0.99,\n","    \"early_stopping_patience\": 10,\n","    \"batch_size\": 512,\n","    \"hidden_size\": 32,\n","    \"encoder_type\": \"rnn\",  # options: rnn / transformer\n","    \"decoder_type\": \"rnn\",  # options: rnn / rnn_attention / transformer\n","    \"attention_type\": \"\",   # options: additive / scaled_dot\n","}\n","rnn_args_l.update(args_dict)\n","\n","print_opts(rnn_args_l)\n","rnn_encode_l, rnn_decoder_l, rnn_losses_l = train(rnn_args_l)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, rnn_encode_l, rnn_decoder_l, None, rnn_args_l\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"markdown","metadata":{"id":"01HsZ6EItc56"},"source":["The code below plots the training and validation losses of each model, as a function of the number of gradient descent iterations. Are there significant differences in the validation performance of each model? (see follow-up questions in handout)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"Qyk_9-Fwtekj","colab":{"base_uri":"https://localhost:8080/","height":337},"executionInfo":{"status":"ok","timestamp":1647557761787,"user_tz":240,"elapsed":813,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}},"outputId":"be46ada7-de96-41a3-c93b-c71ed7ac6183"},"outputs":[{"output_type":"stream","name":"stdout","text":["Plot saved to: /content/content/csc421/a3/content/csc421/a3/loss_plot_gru.pdf\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 720x288 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["save_loss_comparison_gru(rnn_losses_s, rnn_losses_l, rnn_args_s, rnn_args_l, \"gru\")"]},{"cell_type":"markdown","metadata":{"id":"cE4ijaCzneAt"},"source":["Select best performing model, and try translating different sentences by changing the variable TEST_SENTENCE. Identify a failure mode and briefly describe it (see follow-up questions in handout)."]},{"cell_type":"code","execution_count":30,"metadata":{"id":"WrNnz8W1nULf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647558075658,"user_tz":240,"elapsed":347,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}},"outputId":"dcf7d7d6-af62-43bd-be39-68600693a065"},"outputs":[{"output_type":"stream","name":"stdout","text":["source:\t\twonderful watermelon \n","translated:\tonfurepray aterementway\n"]}],"source":["best_encoder = rnn_encode_l  # Replace with rnn_encode_s or rnn_encode_l\n","best_decoder = rnn_decoder_l  # Replace with rnn_decoder_s or rnn_decoder_l\n","best_args = rnn_args_l     # Replace with rnn_args_s or rnn_args_l\n","\n","TEST_SENTENCE = \"wonderful watermelon\"\n","translated = translate_sentence(\n","    TEST_SENTENCE, best_encoder, best_decoder, None, best_args\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"markdown","metadata":{"id":"RWwA6OGqlaTq"},"source":["# Part 2: Attention mechanisms"]},{"cell_type":"markdown","metadata":{"id":"AJSafHSAmu_w"},"source":["## Step 1: Additive attention\n","\n","In the next cell, the [additive attention](https://paperswithcode.com/method/additive-attention) mechanism has been implemented for you. Please take a momement to read through it and understand what it is doing. See the assignment handouts for details."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"AdewEVSMo5jJ","executionInfo":{"status":"ok","timestamp":1647558101542,"user_tz":240,"elapsed":127,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}}},"outputs":[],"source":["class AdditiveAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(AdditiveAttention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","\n","        # A two layer fully-connected network\n","        # hidden_size * 2 --> hidden_size, ReLU, hidden_size --> 1\n","        self.attention_network = nn.Sequential(\n","            nn.Linear(hidden_size * 2, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, 1),\n","        )\n","\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, queries, keys, values):\n","        \"\"\"The forward pass of the additive attention mechanism.\n","\n","        Arguments:\n","            queries: The current decoder hidden state. (batch_size x hidden_size)\n","            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","\n","        Returns:\n","            context: weighted average of the values (batch_size x 1 x hidden_size)\n","            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n","\n","            The attention_weights must be a softmax weighting over the seq_len annotations.\n","        \"\"\"\n","        batch_size = keys.size(0)\n","        expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(\n","            keys\n","        )\n","        concat_inputs = torch.cat([expanded_queries, keys], dim=2)\n","        unnormalized_attention = self.attention_network(concat_inputs)\n","        attention_weights = self.softmax(unnormalized_attention)\n","        context = torch.bmm(attention_weights.transpose(2, 1), values)\n","        return context, attention_weights"]},{"cell_type":"markdown","metadata":{"id":"73_p8d5EmvOJ"},"source":["## Step 2: RNN + additive attention\n","\n","In the next cell, a modification of our `RNNDecoder` that makes use of an additive attention mechanism as been implemented for your. Please take a momement to read through it and understand what it is doing. See the assignment handouts for details."]},{"cell_type":"code","execution_count":32,"metadata":{"id":"RJaABkXrpJSw","executionInfo":{"status":"ok","timestamp":1647558144285,"user_tz":240,"elapsed":83,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}}},"outputs":[],"source":["class RNNAttentionDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, attention_type=\"scaled_dot\"):\n","        super(RNNAttentionDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","\n","        self.rnn = MyGRUCell(input_size=hidden_size * 2, hidden_size=hidden_size)\n","        if attention_type == \"additive\":\n","            self.attention = AdditiveAttention(hidden_size=hidden_size)\n","        elif attention_type == \"scaled_dot\":\n","            self.attention = ScaledDotAttention(hidden_size=hidden_size)\n","\n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, inputs, annotations, hidden_init):\n","        \"\"\"Forward pass of the attention-based decoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n","            annotations: The encoder hidden states for each step of the input.\n","                         sequence. (batch_size x seq_len x hidden_size)\n","            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n","\n","        Returns:\n","            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n","            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n","        \"\"\"\n","\n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","\n","        hiddens = []\n","        attentions = []\n","        h_prev = hidden_init\n","\n","        for i in range(seq_len):\n","            embed_current = embed[\n","                :, i, :\n","            ]  # Get the current time step, across the whole batch\n","            context, attention_weights = self.attention(\n","                h_prev, annotations, annotations\n","            )  # batch_size x 1 x hidden_size\n","            embed_and_context = torch.cat(\n","                [embed_current, context.squeeze(1)], dim=1\n","            )  # batch_size x (2*hidden_size)\n","            h_prev = self.rnn(embed_and_context, h_prev)  # batch_size x hidden_size\n","\n","            hiddens.append(h_prev)\n","            attentions.append(attention_weights)\n","\n","        hiddens = torch.stack(hiddens, dim=1)  # batch_size x seq_len x hidden_size\n","        attentions = torch.cat(attentions, dim=2)  # batch_size x seq_len x seq_len\n","\n","        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n","        return output, attentions"]},{"cell_type":"markdown","metadata":{"id":"vYPae08Io1Fi"},"source":["## Step 3: Training and analysis (with additive attention)\n","\n","Now, run the following cell to train our recurrent encoder-decoder model with additive attention. How does it perform compared to the recurrent encoder-decoder model without attention?"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"ke6t6rCezpZV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647558456616,"user_tz":240,"elapsed":304676,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}},"outputId":"55caf54d-b0de-4614-8dae-86e2c8bc64d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_small                        \n","                                   cuda: 1                                      \n","                                nepochs: 50                                     \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.005                                  \n","                               lr_decay: 0.99                                   \n","                early_stopping_patience: 10                                     \n","                             batch_size: 64                                     \n","                            hidden_size: 64                                     \n","                           encoder_type: rnn                                    \n","                           decoder_type: rnn_attention                          \n","                         attention_type: additive                               \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('tricks', 'ickstray')\n","('trembled', 'embledtray')\n","('novelty', 'oveltynay')\n","('quiet', 'ietquay')\n","('played', 'ayedplay')\n","Num unique word pairs: 3198\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 1.983 | Val loss: 1.816 | Gen: etay-etay-etay-etay- aray-aray-ay indionddinginginging issississisissisissi ouray-oday-oday-oday\n","Epoch:   1 | Train loss: 1.459 | Val loss: 1.507 | Gen: ettay-etetetetetetet ariay onday-odingday-oding issiossiy orday-orday-orday-or\n","Epoch:   2 | Train loss: 1.177 | Val loss: 1.342 | Gen: etay aray ondinday-odinday isway orway\n","Epoch:   3 | Train loss: 0.972 | Val loss: 1.226 | Gen: ehay aray ondonday issinway orway\n","Epoch:   4 | Train loss: 0.815 | Val loss: 1.159 | Gen: ethay-etay-etay-etay iarway onday-oonday-oonday- issway orway\n","Epoch:   5 | Train loss: 0.674 | Val loss: 1.013 | Gen: etay-eay-ay-ay-ay-ay arway onditingingingway isway orway-uringway\n","Epoch:   6 | Train loss: 0.518 | Val loss: 0.958 | Gen: ethay-eay-eay airway onditingcay isway orway\n","Epoch:   7 | Train loss: 0.452 | Val loss: 0.870 | Gen: ethay airway ondingay isway orkway-umingway\n","Epoch:   8 | Train loss: 0.361 | Val loss: 0.794 | Gen: ethay airway ondiditingcay isway orkingway\n","Epoch:   9 | Train loss: 0.285 | Val loss: 0.665 | Gen: ethay airway onditinday isway orkingway\n","Epoch:  10 | Train loss: 0.215 | Val loss: 0.760 | Gen: ethay airway ondiditingcay issway orkingway\n","Epoch:  11 | Train loss: 0.216 | Val loss: 0.822 | Gen: ethay awarway onditintintcay-ointi issay orkway\n","Epoch:  12 | Train loss: 0.245 | Val loss: 0.698 | Gen: etay airway ondintingcay isway orkway-ingway\n","Epoch:  13 | Train loss: 0.187 | Val loss: 0.614 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  14 | Train loss: 0.121 | Val loss: 0.611 | Gen: ethay airway onditiningcay isway orwingway\n","Epoch:  15 | Train loss: 0.100 | Val loss: 0.515 | Gen: ethay airway onditingcay isway orkway-ingway\n","Epoch:  16 | Train loss: 0.077 | Val loss: 0.511 | Gen: ethay airway onditiongay issway orkway-ingway\n","Epoch:  17 | Train loss: 0.087 | Val loss: 0.520 | Gen: ethay airway onditiongay issway orkway-ingway\n","Epoch:  18 | Train loss: 0.065 | Val loss: 0.575 | Gen: ethay airway onditioningcay issway orkway-ingway\n","Epoch:  19 | Train loss: 0.049 | Val loss: 0.499 | Gen: ethay airway onditincy-antiningca issway orkwangway\n","Epoch:  20 | Train loss: 0.049 | Val loss: 0.498 | Gen: ethay airway onditingcay issway orkingway\n","Epoch:  21 | Train loss: 0.048 | Val loss: 0.579 | Gen: ethay airray onditingcay issway orkingway\n","Epoch:  22 | Train loss: 0.054 | Val loss: 0.467 | Gen: ethay airway onditingcay issway orkingway\n","Epoch:  23 | Train loss: 0.026 | Val loss: 0.434 | Gen: ethay airway onditiongcay issway orkingway\n","Epoch:  24 | Train loss: 0.018 | Val loss: 0.436 | Gen: ethay airway onditiongcay isway orkwingway\n","Epoch:  25 | Train loss: 0.027 | Val loss: 0.488 | Gen: ethay airway onditingcay issway orkwingway\n","Epoch:  26 | Train loss: 0.022 | Val loss: 0.429 | Gen: ethay airway onditiongay issway orkingway\n","Epoch:  27 | Train loss: 0.027 | Val loss: 0.733 | Gen: ethay airway onditioningcay issway orkwingway\n","Epoch:  28 | Train loss: 0.059 | Val loss: 0.531 | Gen: ethay airway onditingcay issway orkway\n","Epoch:  29 | Train loss: 0.086 | Val loss: 0.846 | Gen: ethay airway ondingcay isway orkway\n","Epoch:  30 | Train loss: 0.205 | Val loss: 0.867 | Gen: ethay arway ondiditionway isway orkway-ingway\n","Epoch:  31 | Train loss: 0.299 | Val loss: 0.810 | Gen: ethay airway onditiongcay isway orkwingway\n","Epoch:  32 | Train loss: 0.215 | Val loss: 0.627 | Gen: ethay airway onditionctioncay isway orwnway\n","Epoch:  33 | Train loss: 0.119 | Val loss: 0.504 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  34 | Train loss: 0.051 | Val loss: 0.423 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  35 | Train loss: 0.026 | Val loss: 0.396 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  36 | Train loss: 0.014 | Val loss: 0.384 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  37 | Train loss: 0.010 | Val loss: 0.374 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  38 | Train loss: 0.007 | Val loss: 0.372 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  39 | Train loss: 0.006 | Val loss: 0.368 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  40 | Train loss: 0.005 | Val loss: 0.367 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  41 | Train loss: 0.004 | Val loss: 0.361 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  42 | Train loss: 0.004 | Val loss: 0.371 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  43 | Train loss: 0.004 | Val loss: 0.355 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  44 | Train loss: 0.003 | Val loss: 0.356 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  45 | Train loss: 0.003 | Val loss: 0.357 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  46 | Train loss: 0.003 | Val loss: 0.355 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  47 | Train loss: 0.002 | Val loss: 0.354 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  48 | Train loss: 0.002 | Val loss: 0.353 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  49 | Train loss: 0.002 | Val loss: 0.353 | Gen: ethay airway onditioncay isway orkingway\n","Obtained lowest validation loss of: 0.35275705477265135\n","source:\t\tthe air conditioning is working \n","translated:\tethay airway onditioncay isway orkingway\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","rnn_attn_args = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_small\",\n","    \"cuda\": True,\n","    \"nepochs\": 50,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 0.005,\n","    \"lr_decay\": 0.99,\n","    \"early_stopping_patience\": 10,\n","    \"batch_size\": 64,\n","    \"hidden_size\": 64,\n","    \"encoder_type\": \"rnn\",            # options: rnn / transformer\n","    \"decoder_type\": \"rnn_attention\",  # options: rnn / rnn_attention / transformer\n","    \"attention_type\": \"additive\",     # options: additive / scaled_dot\n","}\n","rnn_attn_args.update(args_dict)\n","\n","print_opts(rnn_attn_args)\n","rnn_attn_encoder, rnn_attn_decoder, rnn_attn_losses = train(rnn_attn_args)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"VNVKbLc0ACj_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647558456963,"user_tz":240,"elapsed":358,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}},"outputId":"54d4271a-1f0a-4b26-aa85-ce78910bfecf"},"outputs":[{"output_type":"stream","name":"stdout","text":["source:\t\tthe air conditioning is working \n","translated:\tethay airway onditioncay isway orkingway\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","translated = translate_sentence(\n","    TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"markdown","metadata":{"id":"xq7nhsEio1w-"},"source":["## Step 4: Implement scaled dot-product attention\n","\n","In the next cell, you will implement the [scaled dot-product attention](https://paperswithcode.com/method/scaled) mechanism. See the assignment handouts for details."]},{"cell_type":"code","execution_count":35,"metadata":{"id":"d_j3oY3hqsJQ","executionInfo":{"status":"ok","timestamp":1647558670888,"user_tz":240,"elapsed":138,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}}},"outputs":[],"source":["class ScaledDotAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(ScaledDotAttention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","\n","        self.Q = nn.Linear(hidden_size, hidden_size)\n","        self.K = nn.Linear(hidden_size, hidden_size)\n","        self.V = nn.Linear(hidden_size, hidden_size)\n","        self.softmax = nn.Softmax(dim=1)\n","        self.scaling_factor = torch.rsqrt(\n","            torch.tensor(self.hidden_size, dtype=torch.float)\n","        )\n","\n","    def forward(self, queries, keys, values):\n","        \"\"\"The forward pass of the scaled dot attention mechanism.\n","\n","        Arguments:\n","            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n","            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","\n","        Returns:\n","            context: weighted average of the values (batch_size x k x hidden_size)\n","            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n","\n","            The output must be a softmax weighting over the seq_len annotations.\n","        \"\"\"\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        batch_size = queries.size(0)\n","        queries = queries.view(batch_size, -1, self.hidden_size)\n","        q = self.Q(queries)\n","        k = self.K(keys)\n","        v = self.V(values)\n","        unnormalized_attention = torch.bmm(k, q.transpose(2,1)) * self.scaling_factor\n","        attention_weights = self.softmax(unnormalized_attention)\n","        context = torch.bmm(attention_weights.transpose(2,1),v)\n","        return context, attention_weights"]},{"cell_type":"markdown","metadata":{"id":"unReAOrjo113"},"source":["## Step 5: Implement causal dot-product Attention\n","\n","\n","Now, implement the casual scaled dot-product attention mechanism. It will be very similar to your implementation for `ScaledDotAttention`. The additional step is to mask out the attention to future timesteps so this attention mechanism can be used in a decoder. See the assignment handouts for details."]},{"cell_type":"code","execution_count":36,"metadata":{"id":"ovigzQffrKqj","executionInfo":{"status":"ok","timestamp":1647558673159,"user_tz":240,"elapsed":233,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}}},"outputs":[],"source":["class CausalScaledDotAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(CausalScaledDotAttention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.neg_inf = torch.tensor(-1e7)\n","\n","        self.Q = nn.Linear(hidden_size, hidden_size)\n","        self.K = nn.Linear(hidden_size, hidden_size)\n","        self.V = nn.Linear(hidden_size, hidden_size)\n","        self.softmax = nn.Softmax(dim=1)\n","        self.scaling_factor = torch.rsqrt(\n","            torch.tensor(self.hidden_size, dtype=torch.float)\n","        )\n","\n","    def forward(self, queries, keys, values):\n","        \"\"\"The forward pass of the scaled dot attention mechanism.\n","\n","        Arguments:\n","            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n","            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","\n","        Returns:\n","            context: weighted average of the values (batch_size x k x hidden_size)\n","            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n","\n","            The output must be a softmax weighting over the seq_len annotations.\n","        \"\"\"\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        batch_size = queries.size(0)\n","        queries = queries.view(batch_size, -1, self.hidden_size)\n","        q = self.Q(queries)\n","        k = self.K(keys)\n","        v = self.V(values)\n","        unnormalized_attention = torch.bmm(k, q.transpose(2,1)) * self.scaling_factor\n","        mask = torch.tril(torch.ones_like(unnormalized_attention) * self.neg_inf, diagonal = -1) # tril\n","        unnormalized_attention += mask\n","        attention_weights = self.softmax(unnormalized_attention)\n","        context = torch.bmm(attention_weights.transpose(2,1), v)\n","        return context, attention_weights"]},{"cell_type":"markdown","metadata":{"id":"ZkjHbtvT6Qxs"},"source":["## Step 6: Attention encoder and decoder\n","\n","The following cells provide an implementation of an encoder and decoder that use a single `ScaledDotAttention` block. Please read through them to understand what they are doing."]},{"cell_type":"code","execution_count":37,"metadata":{"id":"yKGNqUaX6RLO","executionInfo":{"status":"ok","timestamp":1647558682044,"user_tz":240,"elapsed":87,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}}},"outputs":[],"source":["class AttentionEncoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, opts):\n","        super(AttentionEncoder, self).__init__()\n","\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.opts = opts\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","\n","        self.self_attention = ScaledDotAttention(\n","                    hidden_size=hidden_size,\n","                )\n","               \n","        self.attention_mlp = nn.Sequential(\n","                                nn.Linear(hidden_size, hidden_size),\n","                                nn.ReLU(),\n","                              )\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward pass of the encoder scaled dot attention.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n","\n","        Returns:\n","            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            None: Used to conform to standard encoder return signature.\n","        \"\"\"\n","        batch_size, seq_len = inputs.size()\n","\n","        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","\n","        annotations = encoded\n","        new_annotations, self_attention_weights = self.self_attention(\n","            annotations, annotations, annotations\n","        )  # batch_size x seq_len x hidden_size\n","        residual_annotations = annotations + new_annotations\n","        new_annotations = self.attention_mlp(residual_annotations)\n","        annotations = residual_annotations + new_annotations\n","\n","        return annotations, None\n"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"vDUvtOee7cMy","executionInfo":{"status":"ok","timestamp":1647558682180,"user_tz":240,"elapsed":2,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}}},"outputs":[],"source":["class AttentionDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size):\n","        super(AttentionDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","\n","        self.self_attention = CausalScaledDotAttention(\n","                                hidden_size=hidden_size,\n","                                )\n","                \n","        self.decoder_attention = ScaledDotAttention(\n","                                  hidden_size=hidden_size,\n","                                  )\n","                \n","        self.attention_mlp = nn.Sequential(\n","                                nn.Linear(hidden_size, hidden_size),\n","                                nn.ReLU(),\n","                              )\n","                \n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","\n","    def forward(self, inputs, annotations, hidden_init):\n","        \"\"\"Forward pass of the attention-based decoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n","            annotations: The encoder hidden states for each step of the input.\n","                         sequence. (batch_size x seq_len x hidden_size)\n","            hidden_init: Not used in the transformer decoder\n","        Returns:\n","            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n","            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n","        \"\"\"\n","\n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","\n","        encoder_attention_weights_list = []\n","        self_attention_weights_list = []\n","        contexts = embed\n","        new_contexts, self_attention_weights = self.self_attention(\n","            contexts, contexts, contexts\n","        )  # batch_size x seq_len x hidden_size\n","        residual_contexts = contexts + new_contexts\n","        new_contexts, encoder_attention_weights = self.decoder_attention(\n","            residual_contexts, annotations, annotations\n","        )  # batch_size x seq_len x hidden_size\n","        residual_contexts = residual_contexts + new_contexts\n","        new_contexts = self.attention_mlp(residual_contexts)\n","        contexts = residual_contexts + new_contexts\n","\n","        encoder_attention_weights_list.append(encoder_attention_weights)\n","        self_attention_weights_list.append(self_attention_weights)\n","\n","        output = self.out(contexts)\n","        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n","        self_attention_weights = torch.stack(self_attention_weights_list)\n","\n","        return output, (encoder_attention_weights, self_attention_weights)"]},{"cell_type":"markdown","metadata":{"id":"B7gJLw5t_rnW"},"source":["## Step 7: Training and analysis (single scaled dot-product attention block)\n","\n","Now, train the following model, with an encoder and decoder each composed a single `ScaledDotAttention` block."]},{"cell_type":"code","execution_count":39,"metadata":{"id":"7MOkZonC8T3f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647558795447,"user_tz":240,"elapsed":111319,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}},"outputId":"04ddb62f-1494-40fb-d24d-b7f7913faf35"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_small                        \n","                                   cuda: 1                                      \n","                                nepochs: 100                                    \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.0005                                 \n","                early_stopping_patience: 100                                    \n","                               lr_decay: 0.99                                   \n","                             batch_size: 64                                     \n","                            hidden_size: 32                                     \n","                           encoder_type: attention                              \n","                           decoder_type: attention                              \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('tricks', 'ickstray')\n","('trembled', 'embledtray')\n","('novelty', 'oveltynay')\n","('quiet', 'ietquay')\n","('played', 'ayedplay')\n","Num unique word pairs: 3198\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 2.874 | Val loss: 2.435 | Gen: ay ay ay ay ay      \n","Epoch:   1 | Train loss: 2.283 | Val loss: 2.174 | Gen: ay iay iiiiiiiiiiiiiiiiiiii iay iay\n","Epoch:   2 | Train loss: 2.096 | Val loss: 2.066 | Gen: eray iiay iay isay iay\n","Epoch:   3 | Train loss: 1.988 | Val loss: 1.987 | Gen: eray iay intintintintintintin isssssssssssssssssss ionay\n","Epoch:   4 | Train loss: 1.902 | Val loss: 1.924 | Gen: eray iay intintintintintintin issssssssssinay isingongongongongong\n","Epoch:   5 | Train loss: 1.830 | Val loss: 1.878 | Gen: eday iay intintintintintintin issssssissssssssssss ingongongonguringuri\n","Epoch:   6 | Train loss: 1.773 | Val loss: 1.837 | Gen: eday iay intintintintintintin isssssisssssssssssss ingonguringuringring\n","Epoch:   7 | Train loss: 1.724 | Val loss: 1.801 | Gen: eday iay ontintintintintintin issssissssssssssssis onguringringringring\n","Epoch:   8 | Train loss: 1.679 | Val loss: 1.768 | Gen: eray iay ontintintintintintin isssissssssssssissss ingay\n","Epoch:   9 | Train loss: 1.637 | Val loss: 1.736 | Gen: eray iay ontintintintintintin ississsssssssissssss ingay\n","Epoch:  10 | Train loss: 1.598 | Val loss: 1.709 | Gen: eray iay ontintintintintintin ississsssssssissssss ingringringrway\n","Epoch:  11 | Train loss: 1.562 | Val loss: 1.685 | Gen: eray iay ontintintintintintin isssisssssssssisssss ingringrway\n","Epoch:  12 | Train loss: 1.530 | Val loss: 1.665 | Gen: eray iay ontintintintintintin issssisssssssssissss ingringrway\n","Epoch:  13 | Train loss: 1.501 | Val loss: 1.646 | Gen: eray iay ontintintintintintin isssissssway ingringway\n","Epoch:  14 | Train loss: 1.474 | Val loss: 1.631 | Gen: eray iay ontintintinday isssisway ingringway\n","Epoch:  15 | Train loss: 1.450 | Val loss: 1.617 | Gen: erchay iay ontinday isway ingway\n","Epoch:  16 | Train loss: 1.428 | Val loss: 1.606 | Gen: erchay iay ondindcay isway ingway\n","Epoch:  17 | Train loss: 1.407 | Val loss: 1.595 | Gen: erchay iay ondindcay isway ingway\n","Epoch:  18 | Train loss: 1.387 | Val loss: 1.587 | Gen: erchay iay ondindcay isway ingway\n","Epoch:  19 | Train loss: 1.369 | Val loss: 1.579 | Gen: erchay iay ondindcay isway ingway\n","Epoch:  20 | Train loss: 1.353 | Val loss: 1.571 | Gen: ethay iay ondindcay isway ingway\n","Epoch:  21 | Train loss: 1.337 | Val loss: 1.563 | Gen: ethay ay ondindway isway ingway\n","Epoch:  22 | Train loss: 1.323 | Val loss: 1.553 | Gen: ethay ay ondindway isway ingway\n","Epoch:  23 | Train loss: 1.308 | Val loss: 1.548 | Gen: ethay ay ondindway isway ingway\n","Epoch:  24 | Train loss: 1.295 | Val loss: 1.539 | Gen: ethay ay ondindway isway ingway\n","Epoch:  25 | Train loss: 1.281 | Val loss: 1.532 | Gen: ethay ay ondindway isway ingway\n","Epoch:  26 | Train loss: 1.268 | Val loss: 1.523 | Gen: ethay ay ondindway isway ingway\n","Epoch:  27 | Train loss: 1.255 | Val loss: 1.517 | Gen: ethay ay ondindway isway ingway\n","Epoch:  28 | Train loss: 1.244 | Val loss: 1.512 | Gen: ethay ay ondindway isway ingway\n","Epoch:  29 | Train loss: 1.232 | Val loss: 1.504 | Gen: ethay ay ondindway isway ingway\n","Epoch:  30 | Train loss: 1.220 | Val loss: 1.498 | Gen: ethay ay ondindway isway ingway\n","Epoch:  31 | Train loss: 1.209 | Val loss: 1.491 | Gen: ethay ay ondindway isway ingway\n","Epoch:  32 | Train loss: 1.198 | Val loss: 1.486 | Gen: ethay ay ondindway isway ingway\n","Epoch:  33 | Train loss: 1.187 | Val loss: 1.483 | Gen: ethay ay ondindway isway ingway\n","Epoch:  34 | Train loss: 1.176 | Val loss: 1.478 | Gen: ethay ay ondindway isway ingway\n","Epoch:  35 | Train loss: 1.167 | Val loss: 1.475 | Gen: ethay ay ondindway isway ingway\n","Epoch:  36 | Train loss: 1.157 | Val loss: 1.473 | Gen: ethay ay ondindway isway ingway\n","Epoch:  37 | Train loss: 1.148 | Val loss: 1.466 | Gen: ethay aray ondindway isway ingway\n","Epoch:  38 | Train loss: 1.138 | Val loss: 1.464 | Gen: ethay aray onginay isway ingway\n","Epoch:  39 | Train loss: 1.129 | Val loss: 1.461 | Gen: ethay aray onginay isway ingway\n","Epoch:  40 | Train loss: 1.120 | Val loss: 1.458 | Gen: ethay aray onginay isway ingway\n","Epoch:  41 | Train loss: 1.111 | Val loss: 1.456 | Gen: ethay aray onginay isway ingway\n","Epoch:  42 | Train loss: 1.103 | Val loss: 1.453 | Gen: ethay aray onginay isway ingray\n","Epoch:  43 | Train loss: 1.095 | Val loss: 1.449 | Gen: ethay aray onginay isway ingray\n","Epoch:  44 | Train loss: 1.088 | Val loss: 1.447 | Gen: ethay aray onginay isway ingray\n","Epoch:  45 | Train loss: 1.081 | Val loss: 1.446 | Gen: ethay aray onginay i ingray\n","Epoch:  46 | Train loss: 1.074 | Val loss: 1.443 | Gen: ethay aray onginay i ingray\n","Epoch:  47 | Train loss: 1.067 | Val loss: 1.441 | Gen: ethay aray onginay i ingray\n","Epoch:  48 | Train loss: 1.062 | Val loss: 1.441 | Gen: ethay aray onginay i ingray\n","Epoch:  49 | Train loss: 1.055 | Val loss: 1.438 | Gen: ethay aray onginay i ingray\n","Epoch:  50 | Train loss: 1.049 | Val loss: 1.438 | Gen: ethay aray onginay i ingray\n","Epoch:  51 | Train loss: 1.044 | Val loss: 1.433 | Gen: ethay aray onginay i ingray\n","Epoch:  52 | Train loss: 1.038 | Val loss: 1.435 | Gen: ethay aray onginay i ingray\n","Epoch:  53 | Train loss: 1.032 | Val loss: 1.431 | Gen: ethay aray onginay i ingray\n","Epoch:  54 | Train loss: 1.027 | Val loss: 1.433 | Gen: ethay aray onginay i ingray\n","Epoch:  55 | Train loss: 1.022 | Val loss: 1.428 | Gen: ethay aray onginay i ingray\n","Epoch:  56 | Train loss: 1.017 | Val loss: 1.431 | Gen: ethay aray onginay i ingray\n","Epoch:  57 | Train loss: 1.013 | Val loss: 1.423 | Gen: ethay aray onginay i ingray\n","Epoch:  58 | Train loss: 1.008 | Val loss: 1.431 | Gen: ethay aray onginay i ingrkinkgway\n","Epoch:  59 | Train loss: 1.003 | Val loss: 1.421 | Gen: ethay aray onginay i ingrkinkgway\n","Epoch:  60 | Train loss: 0.998 | Val loss: 1.429 | Gen: ethay aray onginay i ingrkinkgway\n","Epoch:  61 | Train loss: 0.994 | Val loss: 1.417 | Gen: ethay aray onginay i ingrkinkgway\n","Epoch:  62 | Train loss: 0.990 | Val loss: 1.431 | Gen: ethay aray onginay i ingrkinkgway\n","Epoch:  63 | Train loss: 0.986 | Val loss: 1.409 | Gen: ethay aray onginay i ingrkinkgway\n","Epoch:  64 | Train loss: 0.981 | Val loss: 1.434 | Gen: ethay aray onginay i oway\n","Epoch:  65 | Train loss: 0.979 | Val loss: 1.406 | Gen: ethay aray onginay i ingrkinkgway\n","Epoch:  66 | Train loss: 0.974 | Val loss: 1.435 | Gen: ethay aray onginay i oway\n","Epoch:  67 | Train loss: 0.971 | Val loss: 1.402 | Gen: ethay aray onginay i ingrkinkgway\n","Epoch:  68 | Train loss: 0.966 | Val loss: 1.433 | Gen: ethay aray onginay i oway\n","Epoch:  69 | Train loss: 0.964 | Val loss: 1.400 | Gen: ethay aray onginay i ingrkinkgway\n","Epoch:  70 | Train loss: 0.958 | Val loss: 1.431 | Gen: ethay aray onginay i oway\n","Epoch:  71 | Train loss: 0.956 | Val loss: 1.397 | Gen: ethay aray onginay i ingrkinkgway\n","Epoch:  72 | Train loss: 0.951 | Val loss: 1.429 | Gen: ethay aray onginay i oway\n","Epoch:  73 | Train loss: 0.949 | Val loss: 1.395 | Gen: ethay aray onginay i oway\n","Epoch:  74 | Train loss: 0.944 | Val loss: 1.428 | Gen: ethay aray onginay i oway\n","Epoch:  75 | Train loss: 0.942 | Val loss: 1.395 | Gen: ethay aray onginay i oway\n","Epoch:  76 | Train loss: 0.938 | Val loss: 1.424 | Gen: ethay aray onginay i oway\n","Epoch:  77 | Train loss: 0.936 | Val loss: 1.392 | Gen: ethay aray ondindindgcinday i oway\n","Epoch:  78 | Train loss: 0.931 | Val loss: 1.421 | Gen: ethay aray onginay i oway\n","Epoch:  79 | Train loss: 0.930 | Val loss: 1.391 | Gen: ethay aray ondindindgcinday i oway\n","Epoch:  80 | Train loss: 0.925 | Val loss: 1.416 | Gen: ethay aray onginay i oway\n","Epoch:  81 | Train loss: 0.923 | Val loss: 1.390 | Gen: ethay aray ondindindgcinday i oway\n","Epoch:  82 | Train loss: 0.919 | Val loss: 1.412 | Gen: ethay aray onginay i oway\n","Epoch:  83 | Train loss: 0.918 | Val loss: 1.389 | Gen: ethay aray ondindindgcinday i oway\n","Epoch:  84 | Train loss: 0.913 | Val loss: 1.409 | Gen: ethay aray onginay i oway\n","Epoch:  85 | Train loss: 0.912 | Val loss: 1.387 | Gen: ethay aray ondindindgcinday i oway\n","Epoch:  86 | Train loss: 0.908 | Val loss: 1.404 | Gen: ethay aray ondindindindgcinday i oway\n","Epoch:  87 | Train loss: 0.906 | Val loss: 1.386 | Gen: ethay aray ondindindindgcinday i oway\n","Epoch:  88 | Train loss: 0.902 | Val loss: 1.399 | Gen: ethay aray ondindindindgcinday i oway\n","Epoch:  89 | Train loss: 0.900 | Val loss: 1.384 | Gen: ethay aray ondindindindgcinday i oway\n","Epoch:  90 | Train loss: 0.897 | Val loss: 1.394 | Gen: ethay aray ondindindindgcinday i oway\n","Epoch:  91 | Train loss: 0.895 | Val loss: 1.384 | Gen: ethay aray ondindindindgcinday i oway\n","Epoch:  92 | Train loss: 0.892 | Val loss: 1.390 | Gen: ethay aray ondindindindgcinday i oway\n","Epoch:  93 | Train loss: 0.890 | Val loss: 1.383 | Gen: ethay aray ondindindindgcinday i oway\n","Epoch:  94 | Train loss: 0.887 | Val loss: 1.386 | Gen: ethay aray ondindindindgcinday i oway\n","Epoch:  95 | Train loss: 0.885 | Val loss: 1.380 | Gen: ethay aray ondindindindgcinday i oway\n","Epoch:  96 | Train loss: 0.883 | Val loss: 1.385 | Gen: ethay aray ondindindindgcinday i oway\n","Epoch:  97 | Train loss: 0.881 | Val loss: 1.376 | Gen: ethay aray ondindindindtingcay i oway\n","Epoch:  98 | Train loss: 0.878 | Val loss: 1.381 | Gen: ethay ay ondindindindtingcay i oway\n","Epoch:  99 | Train loss: 0.876 | Val loss: 1.375 | Gen: ethay aray ondindindindtingcay i oway\n","Obtained lowest validation loss of: 1.3751116584647785\n","source:\t\tthe air conditioning is working \n","translated:\tethay aray ondindindindtingcay i oway\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","attention_args_s = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_small\",\n","    \"cuda\": True,\n","    \"nepochs\": 100,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 5e-4,\n","    \"early_stopping_patience\": 100,\n","    \"lr_decay\": 0.99,\n","    \"batch_size\": 64,\n","    \"hidden_size\": 32,\n","    \"encoder_type\": \"attention\",\n","    \"decoder_type\": \"attention\",  # options: rnn / rnn_attention / attention / transformer\n","}\n","attention_args_s.update(args_dict)\n","print_opts(attention_args_s)\n","\n","attention_encoder_s, attention_decoder_s, attention_losses_s = train(attention_args_s)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, attention_encoder_s, attention_decoder_s, None, attention_args_s\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"markdown","metadata":{"id":"9tcpUFKqo2Oi"},"source":["## Step 8: Transformer encoder and decoder\n","\n","The following cells provide an implementation of the transformer encoder and decoder that use your `ScaledDotAttention` and `CausalScaledDotAttention`. Please read through them to understand what they are doing."]},{"cell_type":"code","execution_count":40,"metadata":{"id":"N3B-fWsarlVk","executionInfo":{"status":"ok","timestamp":1647558795519,"user_tz":240,"elapsed":1,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}}},"outputs":[],"source":["class TransformerEncoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n","        super(TransformerEncoder, self).__init__()\n","\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.opts = opts\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","\n","        self.self_attentions = nn.ModuleList(\n","            [\n","                ScaledDotAttention(\n","                    hidden_size=hidden_size,\n","                )\n","                for i in range(self.num_layers)\n","            ]\n","        )\n","        self.attention_mlps = nn.ModuleList(\n","            [\n","                nn.Sequential(\n","                    nn.Linear(hidden_size, hidden_size),\n","                    nn.ReLU(),\n","                )\n","                for i in range(self.num_layers)\n","            ]\n","        )\n","\n","        self.positional_encodings = self.create_positional_encodings()\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward pass of the encoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n","\n","        Returns:\n","            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            None: Used to conform to standard encoder return signature.\n","            None: Used to conform to standard encoder return signature.\n","        \"\"\"\n","        batch_size, seq_len = inputs.size()\n","\n","        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","\n","        # Add positinal embeddings from self.create_positional_encodings. (a'la https://arxiv.org/pdf/1706.03762.pdf, section 3.5)\n","        encoded = encoded + self.positional_encodings[:seq_len]\n","\n","        annotations = encoded\n","        for i in range(self.num_layers):\n","            new_annotations, self_attention_weights = self.self_attentions[i](\n","                annotations, annotations, annotations\n","            )  # batch_size x seq_len x hidden_size\n","            residual_annotations = annotations + new_annotations\n","            new_annotations = self.attention_mlps[i](residual_annotations)\n","            annotations = residual_annotations + new_annotations\n","\n","        # Transformer encoder does not have a last hidden or cell layer.\n","        return annotations, None\n","        # return annotations, None, None\n","    def create_positional_encodings(self, max_seq_len=1000):\n","        \"\"\"Creates positional encodings for the inputs.\n","\n","        Arguments:\n","            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n","\n","        Returns:\n","            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len.\n","        \"\"\"\n","        pos_indices = torch.arange(max_seq_len)[..., None]\n","        dim_indices = torch.arange(self.hidden_size // 2)[None, ...]\n","        exponents = (2 * dim_indices).float() / (self.hidden_size)\n","        trig_args = pos_indices / (10000**exponents)\n","        sin_terms = torch.sin(trig_args)\n","        cos_terms = torch.cos(trig_args)\n","\n","        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n","        pos_encodings[:, 0::2] = sin_terms\n","        pos_encodings[:, 1::2] = cos_terms\n","\n","        if self.opts.cuda:\n","            pos_encodings = pos_encodings.cuda()\n","\n","        return pos_encodings"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"nyvTZFxtrvc6","executionInfo":{"status":"ok","timestamp":1647558795674,"user_tz":240,"elapsed":5,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}}},"outputs":[],"source":["class TransformerDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, num_layers):\n","        super(TransformerDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.num_layers = num_layers\n","\n","        self.self_attentions = nn.ModuleList(\n","            [\n","                CausalScaledDotAttention(\n","                    hidden_size=hidden_size,\n","                )\n","                for i in range(self.num_layers)\n","            ]\n","        )\n","        self.encoder_attentions = nn.ModuleList(\n","            [\n","                ScaledDotAttention(\n","                    hidden_size=hidden_size,\n","                )\n","                for i in range(self.num_layers)\n","            ]\n","        )\n","        self.attention_mlps = nn.ModuleList(\n","            [\n","                nn.Sequential(\n","                    nn.Linear(hidden_size, hidden_size),\n","                    nn.ReLU(),\n","                )\n","                for i in range(self.num_layers)\n","            ]\n","        )\n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","        self.positional_encodings = self.create_positional_encodings()\n","\n","    def forward(self, inputs, annotations, hidden_init):\n","        \"\"\"Forward pass of the attention-based decoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n","            annotations: The encoder hidden states for each step of the input.\n","                         sequence. (batch_size x seq_len x hidden_size)\n","            hidden_init: Not used in the transformer decoder\n","        Returns:\n","            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n","            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n","        \"\"\"\n","\n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","\n","        embed = embed + self.positional_encodings[:seq_len]\n","\n","        encoder_attention_weights_list = []\n","        self_attention_weights_list = []\n","        contexts = embed\n","        for i in range(self.num_layers):\n","            new_contexts, self_attention_weights = self.self_attentions[i](\n","                contexts, contexts, contexts\n","            )  # batch_size x seq_len x hidden_size\n","            residual_contexts = contexts + new_contexts\n","            new_contexts, encoder_attention_weights = self.encoder_attentions[i](\n","                residual_contexts, annotations, annotations\n","            )  # batch_size x seq_len x hidden_size\n","            residual_contexts = residual_contexts + new_contexts\n","            new_contexts = self.attention_mlps[i](residual_contexts)\n","            contexts = residual_contexts + new_contexts\n","\n","            encoder_attention_weights_list.append(encoder_attention_weights)\n","            self_attention_weights_list.append(self_attention_weights)\n","\n","        output = self.out(contexts)\n","        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n","        self_attention_weights = torch.stack(self_attention_weights_list)\n","\n","        return output, (encoder_attention_weights, self_attention_weights)\n","\n","    def create_positional_encodings(self, max_seq_len=1000):\n","        \"\"\"Creates positional encodings for the inputs.\n","\n","        Arguments:\n","            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n","\n","        Returns:\n","            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len.\n","        \"\"\"\n","        pos_indices = torch.arange(max_seq_len)[..., None]\n","        dim_indices = torch.arange(self.hidden_size // 2)[None, ...]\n","        exponents = (2 * dim_indices).float() / (self.hidden_size)\n","        trig_args = pos_indices / (10000**exponents)\n","        sin_terms = torch.sin(trig_args)\n","        cos_terms = torch.cos(trig_args)\n","\n","        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n","        pos_encodings[:, 0::2] = sin_terms\n","        pos_encodings[:, 1::2] = cos_terms\n","\n","        pos_encodings = pos_encodings.cuda()\n","\n","        return pos_encodings"]},{"cell_type":"markdown","metadata":{"id":"29ZjkXTNrUKb"},"source":["\n","## Step 9: Training and analysis (with scaled dot-product attention)\n","\n","Now we will train a (simplified) transformer encoder-decoder model.\n","\n","First, we train our smaller model on the small dataset. Use this model to answer Question 5 in the handout."]},{"cell_type":"code","execution_count":42,"metadata":{"id":"mk8e4KSnuZ8N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647559103163,"user_tz":240,"elapsed":307494,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}},"outputId":"beaf4001-b501-4ed6-a01f-d79bb9bfb06c"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_small                        \n","                                   cuda: 1                                      \n","                                nepochs: 100                                    \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.0005                                 \n","                early_stopping_patience: 100                                    \n","                               lr_decay: 0.99                                   \n","                             batch_size: 64                                     \n","                            hidden_size: 32                                     \n","                           encoder_type: transformer                            \n","                           decoder_type: transformer                            \n","                 num_transformer_layers: 4                                      \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('tricks', 'ickstray')\n","('trembled', 'embledtray')\n","('novelty', 'oveltynay')\n","('quiet', 'ietquay')\n","('played', 'ayedplay')\n","Num unique word pairs: 3198\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 3.174 | Val loss: 2.581 | Gen: attttttttttttttttttt ay inmttttttttttttttttt o iiinnnnng\n","Epoch:   1 | Train loss: 2.245 | Val loss: 2.210 | Gen: etttetetetaytaytty ay-ay inmtinmomay ay-ay ingingingdingooooooo\n","Epoch:   2 | Train loss: 1.928 | Val loss: 1.993 | Gen: etttetay ay-ay ontintiotiooootay ssssay iongringway\n","Epoch:   3 | Train loss: 1.740 | Val loss: 1.874 | Gen: etttay ay otintiotiotay ay ooway-onway\n","Epoch:   4 | Train loss: 1.598 | Val loss: 1.787 | Gen: etttay ay-ay otintiotiontay isssay oouway\n","Epoch:   5 | Train loss: 1.499 | Val loss: 1.653 | Gen: ethway araray otingtiongtiongtay issay oringway\n","Epoch:   6 | Train loss: 1.410 | Val loss: 1.732 | Gen: etetetetetetattthway arararday onononontiongtiongti issssay oronggggggway\n","Epoch:   7 | Train loss: 1.345 | Val loss: 1.581 | Gen: ethway arararay onontiongtiongtay isay oringgggway\n","Epoch:   8 | Train loss: 1.258 | Val loss: 1.560 | Gen: ethway aray ontionday isay orway\n","Epoch:   9 | Train loss: 1.183 | Val loss: 1.504 | Gen: ethay aray ontiontiontiontay isay orway\n","Epoch:  10 | Train loss: 1.114 | Val loss: 1.532 | Gen: eway aray ontiontiontiontay isay orway\n","Epoch:  11 | Train loss: 1.062 | Val loss: 1.602 | Gen: eway aiway indiontiontay isay orway\n","Epoch:  12 | Train loss: 1.033 | Val loss: 1.526 | Gen: eway ariway ondiontiontiontay isay orway\n","Epoch:  13 | Train loss: 0.985 | Val loss: 1.441 | Gen: ethway aray ontiononiontiontay isay orway\n","Epoch:  14 | Train loss: 0.958 | Val loss: 1.470 | Gen: ethway arway ontiontiondway isay orway\n","Epoch:  15 | Train loss: 0.921 | Val loss: 1.408 | Gen: ethway arway ontiondionday iay orway\n","Epoch:  16 | Train loss: 0.854 | Val loss: 1.437 | Gen: eway arway ontiondenentway ay orway\n","Epoch:  17 | Train loss: 0.813 | Val loss: 1.457 | Gen: ethay arway ontiondentay iay orway\n","Epoch:  18 | Train loss: 0.791 | Val loss: 1.434 | Gen: eway arway ondintiontay ay owfay\n","Epoch:  19 | Train loss: 0.757 | Val loss: 1.418 | Gen: ethway arway ondiontiontway isay owfgway\n","Epoch:  20 | Train loss: 0.723 | Val loss: 1.347 | Gen: ethway arway ondindiontiontway isay orwfgway\n","Epoch:  21 | Train loss: 0.696 | Val loss: 1.343 | Gen: ethway arway ondiondiontiondway isay orwfgway\n","Epoch:  22 | Train loss: 0.667 | Val loss: 1.327 | Gen: ewhay ariway ondiondiontiondway isay orwfgway\n","Epoch:  23 | Train loss: 0.645 | Val loss: 1.325 | Gen: ehway arway ondiondiontiondway isay owfgway\n","Epoch:  24 | Train loss: 0.647 | Val loss: 1.449 | Gen: ethway ariray ondiondiondiondway issay orwingggway\n","Epoch:  25 | Train loss: 0.674 | Val loss: 1.285 | Gen: eway ariway ondiondiontiongway isay owingway\n","Epoch:  26 | Train loss: 0.620 | Val loss: 1.424 | Gen: ethway ariway induray isay orwfgway\n","Epoch:  27 | Train loss: 0.625 | Val loss: 1.467 | Gen: eway arway onnndiontionway isay owinway\n","Epoch:  28 | Train loss: 0.587 | Val loss: 1.256 | Gen: ethay arway onnnntiondway isway owfinway\n","Epoch:  29 | Train loss: 0.539 | Val loss: 1.212 | Gen: ethway ariway onnntioniongway isay orwingway\n","Epoch:  30 | Train loss: 0.509 | Val loss: 1.225 | Gen: ewway ariway onnntioniooondway isway owfgwway\n","Epoch:  31 | Train loss: 0.496 | Val loss: 1.177 | Gen: ethway ariway onnndiontioceway isway owfgway\n","Epoch:  32 | Train loss: 0.470 | Val loss: 1.174 | Gen: ethway ariway onndiontionceway isay owfgway\n","Epoch:  33 | Train loss: 0.455 | Val loss: 1.152 | Gen: ethway ariway onndiontionceway isway owingway\n","Epoch:  34 | Train loss: 0.443 | Val loss: 1.167 | Gen: ethway ariway ondiontiongceway isay orwinggway\n","Epoch:  35 | Train loss: 0.427 | Val loss: 1.176 | Gen: ethway ariway onndiongceway isway owfgway\n","Epoch:  36 | Train loss: 0.419 | Val loss: 1.262 | Gen: ehway ariway ondiontionceway isay owingway\n","Epoch:  37 | Train loss: 0.433 | Val loss: 1.266 | Gen: ethay ariway ondiontiongway isway owiggway\n","Epoch:  38 | Train loss: 0.449 | Val loss: 1.249 | Gen: ehthwhay ariway ondioncidway isway orwigggway\n","Epoch:  39 | Train loss: 0.435 | Val loss: 1.178 | Gen: ethway ariway ondidionnceway isway orwinggway\n","Epoch:  40 | Train loss: 0.407 | Val loss: 1.363 | Gen: ehthay arhfay ondioncenceway isway owiggway\n","Epoch:  41 | Train loss: 0.425 | Val loss: 1.174 | Gen: ehway ariway ondioncidpentway isay orwiggway\n","Epoch:  42 | Train loss: 0.377 | Val loss: 1.223 | Gen: ehthay ariway ondidiontiocay isay owigway\n","Epoch:  43 | Train loss: 0.356 | Val loss: 1.188 | Gen: ehthay arhay ondioncidway isay owingway\n","Epoch:  44 | Train loss: 0.339 | Val loss: 1.164 | Gen: ehthay ariway ondiontionceway isay owinggway\n","Epoch:  45 | Train loss: 0.322 | Val loss: 1.257 | Gen: ehthay arhay onnodiontioncay isway owingway\n","Epoch:  46 | Train loss: 0.320 | Val loss: 1.149 | Gen: ehthay arway ondidionncay isway owingway\n","Epoch:  47 | Train loss: 0.303 | Val loss: 1.215 | Gen: ehthay arhay ondioncidway isway owingway\n","Epoch:  48 | Train loss: 0.300 | Val loss: 1.228 | Gen: ehthay arway ondidionncay isway owingway\n","Epoch:  49 | Train loss: 0.301 | Val loss: 1.175 | Gen: ehthay arway ondidionnnceway isway owinggway\n","Epoch:  50 | Train loss: 0.329 | Val loss: 1.285 | Gen: ehthay arway ondioncidpway isway orwinggway\n","Epoch:  51 | Train loss: 0.336 | Val loss: 1.239 | Gen: ehthay arway ondiontioncay isay okfgway\n","Epoch:  52 | Train loss: 0.317 | Val loss: 1.325 | Gen: ethay ariay ondidinntway isway orwingway\n","Epoch:  53 | Train loss: 0.296 | Val loss: 1.211 | Gen: eway arway ondiontionceway isway owingway\n","Epoch:  54 | Train loss: 0.269 | Val loss: 1.146 | Gen: ehthay arway ondiontionceway isway orkfgway\n","Epoch:  55 | Train loss: 0.251 | Val loss: 1.160 | Gen: ehthay arway ondidionnceway isway orkfgway\n","Epoch:  56 | Train loss: 0.241 | Val loss: 1.162 | Gen: ehthay arway ondidionnceway isway okfgwway\n","Epoch:  57 | Train loss: 0.234 | Val loss: 1.166 | Gen: ehthay arway ondidionnceway isway okfgwway\n","Epoch:  58 | Train loss: 0.229 | Val loss: 1.164 | Gen: ehthay arway ondidionnceway isway okfgway\n","Epoch:  59 | Train loss: 0.221 | Val loss: 1.151 | Gen: ethay arway ondidionnchway isway okfgwway\n","Epoch:  60 | Train loss: 0.218 | Val loss: 1.186 | Gen: ethay arway ondidionncay isway okfgwway\n","Epoch:  61 | Train loss: 0.235 | Val loss: 1.190 | Gen: ehthay arway ondidiontioncay isway okfgwway\n","Epoch:  62 | Train loss: 0.239 | Val loss: 1.333 | Gen: ethay arway ondidinnncdway isway orwinggway\n","Epoch:  63 | Train loss: 0.292 | Val loss: 1.464 | Gen: ehthay ararway ondioncidway isay owingway\n","Epoch:  64 | Train loss: 0.246 | Val loss: 1.223 | Gen: ethay arway ondiontionceway isway owingway\n","Epoch:  65 | Train loss: 0.228 | Val loss: 1.161 | Gen: ethay arway ondidincionway isway orkfingway\n","Epoch:  66 | Train loss: 0.210 | Val loss: 1.141 | Gen: ethay arway ondidionncionway isway okfingway\n","Epoch:  67 | Train loss: 0.190 | Val loss: 1.151 | Gen: ethay arway ondidincionway isway okfingway\n","Epoch:  68 | Train loss: 0.184 | Val loss: 1.162 | Gen: ethay arway ondidionncay isway okfingway\n","Epoch:  69 | Train loss: 0.179 | Val loss: 1.153 | Gen: ethay arway ondidinncionway isway okfgwway\n","Epoch:  70 | Train loss: 0.173 | Val loss: 1.171 | Gen: ethay arway ondidinncionway isway okfingway\n","Epoch:  71 | Train loss: 0.168 | Val loss: 1.176 | Gen: ethay arway ondidinncionway isway okfingway\n","Epoch:  72 | Train loss: 0.164 | Val loss: 1.198 | Gen: ethay arway ondidincionway isway okfingway\n","Epoch:  73 | Train loss: 0.159 | Val loss: 1.205 | Gen: ethay arway ondidinncionway isway okfingway\n","Epoch:  74 | Train loss: 0.155 | Val loss: 1.230 | Gen: ethay arway ondidincionway isway okfingway\n","Epoch:  75 | Train loss: 0.151 | Val loss: 1.233 | Gen: ethay arway ondidinncionway isway okkfrway\n","Epoch:  76 | Train loss: 0.147 | Val loss: 1.255 | Gen: ethay arway ondidincionway isway okkfrway\n","Epoch:  77 | Train loss: 0.143 | Val loss: 1.262 | Gen: ethway arway ondidinnciconway isway okkfgway\n","Epoch:  78 | Train loss: 0.141 | Val loss: 1.273 | Gen: ethay arway ondidionciconway isway okkingway\n","Epoch:  79 | Train loss: 0.138 | Val loss: 1.282 | Gen: ethway arway ondidinncionway isway okkfgway\n","Epoch:  80 | Train loss: 0.138 | Val loss: 1.298 | Gen: ethway arway ondidionceway isway okkfingway\n","Epoch:  81 | Train loss: 0.161 | Val loss: 1.445 | Gen: ethway arway ondidioncionway isway okfingway\n","Epoch:  82 | Train loss: 0.261 | Val loss: 1.472 | Gen: ethway arway ondioncidpway isway okkingway\n","Epoch:  83 | Train loss: 0.371 | Val loss: 1.316 | Gen: ehthay arway ondidioncictingway isay okjkingway\n","Epoch:  84 | Train loss: 0.358 | Val loss: 1.160 | Gen: ethway arway ondivontioncay isway okfingway\n","Epoch:  85 | Train loss: 0.265 | Val loss: 1.122 | Gen: ethway arway ondivivintwingway isway okfringway\n","Epoch:  86 | Train loss: 0.220 | Val loss: 1.109 | Gen: ethay arway ondidinncay isway okfingway\n","Epoch:  87 | Train loss: 0.176 | Val loss: 1.032 | Gen: ethay arway ondidinncionway isway okfingway\n","Epoch:  88 | Train loss: 0.154 | Val loss: 1.026 | Gen: ethay arway ondiviontioncay isway okfingway\n","Epoch:  89 | Train loss: 0.142 | Val loss: 1.023 | Gen: ethay arway ondiviontway isway okfingway\n","Epoch:  90 | Train loss: 0.134 | Val loss: 1.030 | Gen: ethay arway ondiviontway isway okfringway\n","Epoch:  91 | Train loss: 0.128 | Val loss: 1.041 | Gen: ethay arway ondiviontway isway okfringway\n","Epoch:  92 | Train loss: 0.123 | Val loss: 1.052 | Gen: ethay arway ondiviontway isway okfringway\n","Epoch:  93 | Train loss: 0.119 | Val loss: 1.062 | Gen: ethay arway ondiviontway isway okfringway\n","Epoch:  94 | Train loss: 0.115 | Val loss: 1.071 | Gen: ethay arway ondiviontway isway okkingway\n","Epoch:  95 | Train loss: 0.111 | Val loss: 1.081 | Gen: ethay arway ondiviontway isway okkingway\n","Epoch:  96 | Train loss: 0.108 | Val loss: 1.090 | Gen: ethay arway ondiviontway isway okkingway\n","Epoch:  97 | Train loss: 0.105 | Val loss: 1.101 | Gen: ethay arway ondidinncionway isway okkingway\n","Epoch:  98 | Train loss: 0.102 | Val loss: 1.108 | Gen: ethay arway ondidinncionway isway okkingway\n","Epoch:  99 | Train loss: 0.100 | Val loss: 1.116 | Gen: ethay arway ondidinncionway isway okkingway\n","Obtained lowest validation loss of: 1.0229302943599494\n","source:\t\tthe air conditioning is working \n","translated:\tethay arway ondidinncionway isway okkingway\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","trans32_args_s = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_small\",\n","    \"cuda\": True,\n","    \"nepochs\": 100,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 5e-4,\n","    \"early_stopping_patience\": 100,\n","    \"lr_decay\": 0.99,\n","    \"batch_size\": 64,\n","    \"hidden_size\": 32,\n","    \"encoder_type\": \"transformer\",\n","    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n","    \"num_transformer_layers\": 4,\n","}\n","trans32_args_s.update(args_dict)\n","print_opts(trans32_args_s)\n","\n","trans32_encoder_s, trans32_decoder_s, trans32_losses_s = train(trans32_args_s)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"l28mKuZxvaRT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647559103437,"user_tz":240,"elapsed":284,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}},"outputId":"84fc2e50-a942-4167-87e3-2c5b90dc9b42"},"outputs":[{"output_type":"stream","name":"stdout","text":["source:\t\tthe air conditioning is working \n","translated:\tethay arway ondidinncionway isway okkingway\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","translated = translate_sentence(\n","    TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"markdown","metadata":{"id":"0L8EqLYFu48H"},"source":["In the following cells, we investigate the effects of increasing model size and dataset size on the training / validation curves and generalization of the Transformer. We will increase hidden size to 64, and also increase dataset size. Include the best achieved validation loss in your report."]},{"cell_type":"code","execution_count":44,"metadata":{"id":"FdZO69DozuUu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647559426580,"user_tz":240,"elapsed":323145,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}},"outputId":"ca0ee713-02ae-4095-b98a-98b66f2d8708"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_large                        \n","                                   cuda: 1                                      \n","                                nepochs: 100                                    \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.0005                                 \n","                early_stopping_patience: 10                                     \n","                               lr_decay: 0.99                                   \n","                             batch_size: 512                                    \n","                            hidden_size: 32                                     \n","                           encoder_type: transformer                            \n","                           decoder_type: transformer                            \n","                 num_transformer_layers: 3                                      \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('merlin', 'erlinmay')\n","('lq', 'lqay')\n","('fiddle', 'iddlefay')\n","('applied', 'appliedway')\n","('multicast', 'ulticastmay')\n","Num unique word pairs: 22402\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 3.099 | Val loss: 2.372 | Gen: ay ay eyyyyyyyyyyyyyyyyyyy yyyyyyyy ayyyyy\n","Epoch:   1 | Train loss: 2.166 | Val loss: 2.104 | Gen: esay ay-ay intinintay-intay-ina issssssay ay-ay-ay-ay-ay-ay-ay\n","Epoch:   2 | Train loss: 1.917 | Val loss: 1.949 | Gen: eway ay inininintay-inininay isssssay onway-ay-ay-ay-ay-ay\n","Epoch:   3 | Train loss: 1.758 | Val loss: 1.823 | Gen: eway ay-ay intinintay-intay isssssay onsay-inway-onay\n","Epoch:   4 | Train loss: 1.636 | Val loss: 1.745 | Gen: ehay ay onininintintntay isay ongnsay\n","Epoch:   5 | Train loss: 1.540 | Val loss: 1.704 | Gen: ehway ay onininintintintay isay ongngay\n","Epoch:   6 | Train loss: 1.471 | Val loss: 1.647 | Gen: ehay uay ongnintintntntay isay ongngay\n","Epoch:   7 | Train loss: 1.385 | Val loss: 1.644 | Gen: ehay iay ongngngnay-ingnay isay ongngngay\n","Epoch:   8 | Train loss: 1.315 | Val loss: 1.551 | Gen: ehay ay ongningngngnnnay isay ongngngay\n","Epoch:   9 | Train loss: 1.253 | Val loss: 1.603 | Gen: eway iay ongngngnay-ingnay isay ongngngnway\n","Epoch:  10 | Train loss: 1.223 | Val loss: 1.519 | Gen: ehay ay ongngninay-y isway ongngnway\n","Epoch:  11 | Train loss: 1.150 | Val loss: 1.487 | Gen: ehay ay ongngay-ingngnay isway ongngngay\n","Epoch:  12 | Train loss: 1.106 | Val loss: 1.459 | Gen: ehthay ay ongngngay-y isway ongngingngngnway\n","Epoch:  13 | Train loss: 1.074 | Val loss: 1.404 | Gen: ehthay airway oningninay isway ongnginway\n","Epoch:  14 | Train loss: 1.036 | Val loss: 1.365 | Gen: eheway arway oningningnay isway ongingray\n","Epoch:  15 | Train loss: 0.981 | Val loss: 1.361 | Gen: ehthay aiway ontingningngnay isway ongingway\n","Epoch:  16 | Train loss: 0.953 | Val loss: 1.392 | Gen: ehay ay ondindinay isay ongingway\n","Epoch:  17 | Train loss: 0.925 | Val loss: 1.316 | Gen: eway iaray ondindingay isway ongingway\n","Epoch:  18 | Train loss: 0.902 | Val loss: 1.309 | Gen: ethay ay ondindintingngnay isway ongingway\n","Epoch:  19 | Train loss: 0.873 | Val loss: 1.368 | Gen: ehay arway ondindinay isway ongay\n","Epoch:  20 | Train loss: 0.860 | Val loss: 1.288 | Gen: ethay ay ondindintingcay isway ongingray\n","Epoch:  21 | Train loss: 0.823 | Val loss: 1.236 | Gen: ehthay arway ondindintingcay isway ongingray\n","Epoch:  22 | Train loss: 0.799 | Val loss: 1.232 | Gen: ethay ay ondindiongray isway ongingway\n","Epoch:  23 | Train loss: 0.779 | Val loss: 1.253 | Gen: ethay arway ondintintiongnay isway orway\n","Epoch:  24 | Train loss: 0.763 | Val loss: 1.360 | Gen: etay ay onditionay isway orway\n","Epoch:  25 | Train loss: 0.747 | Val loss: 1.204 | Gen: ethay ariway ondintiontingnay isway orway\n","Epoch:  26 | Train loss: 0.718 | Val loss: 1.188 | Gen: ethay ay onditionay-y isway orkway\n","Epoch:  27 | Train loss: 0.685 | Val loss: 1.125 | Gen: ethay ariway onditionntingnay isway orkway\n","Epoch:  28 | Train loss: 0.662 | Val loss: 1.135 | Gen: ethay arway onditionngray isway orkway\n","Epoch:  29 | Train loss: 0.643 | Val loss: 1.104 | Gen: ethay airway onditionay isway orkway\n","Epoch:  30 | Train loss: 0.631 | Val loss: 1.137 | Gen: ethay arway onditionay-y isway orkway\n","Epoch:  31 | Train loss: 0.614 | Val loss: 1.096 | Gen: ethay ariway onditiondingnay isway orkway\n","Epoch:  32 | Train loss: 0.597 | Val loss: 1.155 | Gen: ethay arway onditiondingcay isway orkway\n","Epoch:  33 | Train loss: 0.587 | Val loss: 1.077 | Gen: ethay ariway onditiontingnay isway orkway\n","Epoch:  34 | Train loss: 0.578 | Val loss: 1.108 | Gen: ethay arway onditiondingnay isway orkingway\n","Epoch:  35 | Train loss: 0.568 | Val loss: 1.058 | Gen: ethay ariway onditioningcay isway orkingway\n","Epoch:  36 | Train loss: 0.555 | Val loss: 1.072 | Gen: ethay ariway ondiditiongnay isway orkingway\n","Epoch:  37 | Train loss: 0.532 | Val loss: 1.041 | Gen: ethay ariway onditionngcay isway orkingway\n","Epoch:  38 | Train loss: 0.516 | Val loss: 1.056 | Gen: ethay ariway ondiditiongway isway orkingway\n","Epoch:  39 | Train loss: 0.515 | Val loss: 1.009 | Gen: ethay ariway ondidindiongcay isway orkingway\n","Epoch:  40 | Train loss: 0.514 | Val loss: 0.991 | Gen: ethay ariway ondidonicay isway orkingway\n","Epoch:  41 | Train loss: 0.511 | Val loss: 1.044 | Gen: ethay ariway ondidintiongcay isway orkingway\n","Epoch:  42 | Train loss: 0.504 | Val loss: 1.091 | Gen: ehtay ariway ondidiondctingay isway orkingway\n","Epoch:  43 | Train loss: 0.481 | Val loss: 0.893 | Gen: ehtay ariway ondidonintiongway isway orkingway\n","Epoch:  44 | Train loss: 0.454 | Val loss: 0.880 | Gen: ehtay ariway ondidioningcay isway orkingway\n","Epoch:  45 | Train loss: 0.431 | Val loss: 0.874 | Gen: ehtay ariway ondidioncingcay isway orkingway\n","Epoch:  46 | Train loss: 0.415 | Val loss: 0.864 | Gen: ethay ariway ondiditiongcay isway orkingway\n","Epoch:  47 | Train loss: 0.405 | Val loss: 0.862 | Gen: ehtay ariway ondidionitionngay isway orkingway\n","Epoch:  48 | Train loss: 0.397 | Val loss: 0.870 | Gen: ethay ariway ondiditiongcay isway orkingway\n","Epoch:  49 | Train loss: 0.383 | Val loss: 0.845 | Gen: ethay ariway ondiditionncay isway orkingway\n","Epoch:  50 | Train loss: 0.372 | Val loss: 0.864 | Gen: ethay ariway ondiditiongcay isway orkingway\n","Epoch:  51 | Train loss: 0.364 | Val loss: 0.839 | Gen: ethay ariway ondiditionncay isway orkingway\n","Epoch:  52 | Train loss: 0.357 | Val loss: 0.865 | Gen: ethay ariway ondiditiongcay isway orkingway\n","Epoch:  53 | Train loss: 0.351 | Val loss: 0.842 | Gen: ethay ariway ondiditionionngcay isway orkingway\n","Epoch:  54 | Train loss: 0.346 | Val loss: 0.851 | Gen: ethay ariway ondiditiongcay isway orkingway\n","Epoch:  55 | Train loss: 0.339 | Val loss: 0.825 | Gen: ethay ariway ondiditiongcay isway orkingway\n","Epoch:  56 | Train loss: 0.334 | Val loss: 0.855 | Gen: ethay ariway ondidiniongcay isway orkingway\n","Epoch:  57 | Train loss: 0.338 | Val loss: 0.890 | Gen: ethay airway ondidinicigway isway orkingway\n","Epoch:  58 | Train loss: 0.369 | Val loss: 0.907 | Gen: ethay ariway ondiditingcay isway orkingway\n","Epoch:  59 | Train loss: 0.399 | Val loss: 0.805 | Gen: ethay ariway onditiniongcay isway orkingway\n","Epoch:  60 | Train loss: 0.353 | Val loss: 0.919 | Gen: ethay ariway ondidonitiongcay isway orkingway\n","Epoch:  61 | Train loss: 0.363 | Val loss: 0.872 | Gen: ethay airway onditionicay isway orkingway\n","Epoch:  62 | Train loss: 0.355 | Val loss: 0.800 | Gen: ethay ariway onditioningcay isway orkingway\n","Epoch:  63 | Train loss: 0.316 | Val loss: 0.771 | Gen: ethay airway onditioniongcay isway orkingway\n","Epoch:  64 | Train loss: 0.289 | Val loss: 0.755 | Gen: ethay ariway onditioningcay isway orkingway\n","Epoch:  65 | Train loss: 0.279 | Val loss: 0.749 | Gen: ethay ariway onditioningcay isway orkingway\n","Epoch:  66 | Train loss: 0.272 | Val loss: 0.750 | Gen: ethay ariway onditiniongcay isway orkingway\n","Epoch:  67 | Train loss: 0.266 | Val loss: 0.749 | Gen: ethay ariway onditiniongcay isway orkingway\n","Epoch:  68 | Train loss: 0.261 | Val loss: 0.748 | Gen: ethay ariway onditiniongcay isway orkingway\n","Epoch:  69 | Train loss: 0.256 | Val loss: 0.747 | Gen: ethay ariway onditiniongcay isway orkingway\n","Epoch:  70 | Train loss: 0.251 | Val loss: 0.746 | Gen: ethay ariway onditiniongcay isway orkingway\n","Epoch:  71 | Train loss: 0.246 | Val loss: 0.746 | Gen: ethay ariway onditiniongcay isway orkingway\n","Epoch:  72 | Train loss: 0.242 | Val loss: 0.744 | Gen: ethay ariway onditiniongcay isway orkingway\n","Epoch:  73 | Train loss: 0.237 | Val loss: 0.746 | Gen: ethay ariway onditiniongcay isway orkingway\n","Epoch:  74 | Train loss: 0.233 | Val loss: 0.745 | Gen: ethay ariway onditiniongcay isway orkingway\n","Epoch:  75 | Train loss: 0.229 | Val loss: 0.745 | Gen: ethay ariway onditiniongcay isway orkingway\n","Epoch:  76 | Train loss: 0.225 | Val loss: 0.743 | Gen: ethay ariway onditiniongcay isway orkingway\n","Epoch:  77 | Train loss: 0.221 | Val loss: 0.748 | Gen: ethay ariway onditiniongcay isway orkingway\n","Epoch:  78 | Train loss: 0.217 | Val loss: 0.745 | Gen: ethay ariway onditiniongcay isway orkingway\n","Epoch:  79 | Train loss: 0.213 | Val loss: 0.750 | Gen: ethay airway onditinioniongcay isway orkingway\n","Epoch:  80 | Train loss: 0.209 | Val loss: 0.746 | Gen: ethay ariway onditinioniongcay isway orkingway\n","Epoch:  81 | Train loss: 0.205 | Val loss: 0.750 | Gen: ethay airway onditinioniongcay isway orkingway\n","Epoch:  82 | Train loss: 0.201 | Val loss: 0.750 | Gen: ethay ariway onditinioniongcay isway orkingway\n","Epoch:  83 | Train loss: 0.198 | Val loss: 0.752 | Gen: ethay airway onditinioniongcay isway orkingway\n","Epoch:  84 | Train loss: 0.205 | Val loss: 0.813 | Gen: ethay airway onditionicingcay isway orkingway\n","Epoch:  85 | Train loss: 0.335 | Val loss: 0.889 | Gen: ehtay ariway ondiditingcay isway orkingway\n","Epoch:  86 | Train loss: 0.291 | Val loss: 0.939 | Gen: ethay airway ondoitiningcay isway ourkingway\n","Epoch:  87 | Train loss: 0.260 | Val loss: 0.682 | Gen: ethay ariway ondidiniongcay isway orkingway\n","Epoch:  88 | Train loss: 0.209 | Val loss: 0.705 | Gen: ethay ariway onditinioniongcay isway orkingway\n","Epoch:  89 | Train loss: 0.195 | Val loss: 0.693 | Gen: ethay ariway onditinioniongcay isway orkingway\n","Epoch:  90 | Train loss: 0.189 | Val loss: 0.690 | Gen: ethay ariway onditinioniongcay isway orkingway\n","Epoch:  91 | Train loss: 0.184 | Val loss: 0.689 | Gen: ethay ariway onditiniongcay isway orkingway\n","Epoch:  92 | Train loss: 0.180 | Val loss: 0.687 | Gen: ethay ariway onditiniongcay isway orkingway\n","Epoch:  93 | Train loss: 0.176 | Val loss: 0.687 | Gen: ethay ariway onditiniongcay isway orkingway\n","Epoch:  94 | Train loss: 0.173 | Val loss: 0.685 | Gen: ethay ariway onditiniongcay isway orkingway\n","Epoch:  95 | Train loss: 0.170 | Val loss: 0.684 | Gen: ethay ariway onditiniongcay isway orkingway\n","Epoch:  96 | Train loss: 0.168 | Val loss: 0.681 | Gen: ethay airway onditiniongcay isway orkingway\n","Epoch:  97 | Train loss: 0.165 | Val loss: 0.680 | Gen: ethay airway onditiniongcay isway orkingway\n","Epoch:  98 | Train loss: 0.162 | Val loss: 0.679 | Gen: ethay airway onditiniongcay isway orkingway\n","Epoch:  99 | Train loss: 0.160 | Val loss: 0.679 | Gen: ethay airway onditiniongcay isway orkingway\n","Obtained lowest validation loss of: 0.6785160089628055\n","source:\t\tthe air conditioning is working \n","translated:\tethay airway onditiniongcay isway orkingway\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","trans32_args_l = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_large\",  # Increased data set size\n","    \"cuda\": True,\n","    \"nepochs\": 100,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 5e-4,\n","    \"early_stopping_patience\": 10,\n","    \"lr_decay\": 0.99,\n","    \"batch_size\": 512,\n","    \"hidden_size\": 32,\n","    \"encoder_type\": \"transformer\",\n","    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n","    \"num_transformer_layers\": 3,\n","}\n","trans32_args_l.update(args_dict)\n","print_opts(trans32_args_l)\n","\n","trans32_encoder_l, trans32_decoder_l, trans32_losses_l = train(trans32_args_l)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, trans32_encoder_l, trans32_decoder_l, None, trans32_args_l\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"SmoTgrDcr_dw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647559548557,"user_tz":240,"elapsed":121987,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}},"outputId":"3068c10f-e245-4b15-a6ca-03c544363407"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_small                        \n","                                   cuda: 1                                      \n","                                nepochs: 50                                     \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.0005                                 \n","                early_stopping_patience: 20                                     \n","                               lr_decay: 0.99                                   \n","                             batch_size: 64                                     \n","                            hidden_size: 64                                     \n","                           encoder_type: transformer                            \n","                           decoder_type: transformer                            \n","                 num_transformer_layers: 3                                      \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('tricks', 'ickstray')\n","('trembled', 'embledtray')\n","('novelty', 'oveltynay')\n","('quiet', 'ietquay')\n","('played', 'ayedplay')\n","Num unique word pairs: 3198\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 2.541 | Val loss: 1.987 | Gen: ay aray inininingongotingay issssay ongay-ay-ongay-ay-ay\n","Epoch:   1 | Train loss: 1.727 | Val loss: 1.724 | Gen: ay aray ongiongay iay ongay\n","Epoch:   2 | Train loss: 1.467 | Val loss: 1.651 | Gen: ty arctray ongiongdiongdgway isssssss ogrgway\n","Epoch:   3 | Train loss: 1.291 | Val loss: 1.463 | Gen: ay aray ondiongiongiongway ay orgway\n","Epoch:   4 | Train loss: 1.143 | Val loss: 1.359 | Gen: tety aray ontiongiongtitingtin issay oringway\n","Epoch:   5 | Train loss: 0.980 | Val loss: 1.373 | Gen: ehay aray oncoiooooooooioioioi isay oingway\n","Epoch:   6 | Train loss: 0.858 | Val loss: 1.178 | Gen: ehtehay iray ondioniongioootiiiii isay oingway\n","Epoch:   7 | Train loss: 0.750 | Val loss: 1.197 | Gen: ehthay aray ondiiiiiiongway asay oringway\n","Epoch:   8 | Train loss: 0.691 | Val loss: 1.082 | Gen: ehthay irayway ondioniongay isay ooorway\n","Epoch:   9 | Train loss: 0.605 | Val loss: 1.102 | Gen: eway iraway ondiioniongay isay oringway\n","Epoch:  10 | Train loss: 0.557 | Val loss: 1.111 | Gen: eeheway rrray onditingingway isway orkingway\n","Epoch:  11 | Train loss: 0.501 | Val loss: 1.062 | Gen: ethay airway onditiongiotiongway isway orkingway\n","Epoch:  12 | Train loss: 0.461 | Val loss: 1.129 | Gen: eetay arway onditiongiotiotionay asay oroingway\n","Epoch:  13 | Train loss: 0.459 | Val loss: 0.951 | Gen: ethay ariway onditingcay isay orkingway\n","Epoch:  14 | Train loss: 0.404 | Val loss: 1.058 | Gen: ay arayway ondititiongcay isay orkway\n","Epoch:  15 | Train loss: 0.395 | Val loss: 0.964 | Gen: eway ariway onditiongcay isayway orkingway\n","Epoch:  16 | Train loss: 0.338 | Val loss: 0.825 | Gen: ethay arrway onditingcay isway orkingway\n","Epoch:  17 | Train loss: 0.286 | Val loss: 0.900 | Gen: etway ariway onditiongcay isway orgingway\n","Epoch:  18 | Train loss: 0.242 | Val loss: 0.858 | Gen: ethay arrway onditingcay sway orkway\n","Epoch:  19 | Train loss: 0.218 | Val loss: 0.868 | Gen: ethay airway onditiongcay isay orkwingway\n","Epoch:  20 | Train loss: 0.206 | Val loss: 0.907 | Gen: ethay arway onditingcay asay orkway\n","Epoch:  21 | Train loss: 0.208 | Val loss: 0.839 | Gen: ethay airway onditiongcay isay orkway\n","Epoch:  22 | Train loss: 0.181 | Val loss: 0.792 | Gen: ethay arway onditingcay isway orkway\n","Epoch:  23 | Train loss: 0.176 | Val loss: 0.809 | Gen: ethay airway onditiongway isay orkwingway\n","Epoch:  24 | Train loss: 0.180 | Val loss: 0.883 | Gen: ethay arway onditiongcay isway orkway\n","Epoch:  25 | Train loss: 0.178 | Val loss: 0.797 | Gen: eethay ariway ondidiongcay isway orkway\n","Epoch:  26 | Train loss: 0.124 | Val loss: 0.667 | Gen: ethay ariway onnditiongcay isway orkwingway\n","Epoch:  27 | Train loss: 0.097 | Val loss: 0.720 | Gen: eethay airway onditionigcay isay orkingway\n","Epoch:  28 | Train loss: 0.086 | Val loss: 0.659 | Gen: ethay arrway onditionigcay isway orkwingway\n","Epoch:  29 | Train loss: 0.081 | Val loss: 0.789 | Gen: ethay arirway onditioningcay isay orgingway\n","Epoch:  30 | Train loss: 0.090 | Val loss: 0.699 | Gen: ethay arway onditioningcay isway orkwingway\n","Epoch:  31 | Train loss: 0.081 | Val loss: 0.896 | Gen: eehay airiway onnditiongcay isay orkwingway\n","Epoch:  32 | Train loss: 0.133 | Val loss: 0.851 | Gen: ethay ariway ondiditingcay isway orkwingway\n","Epoch:  33 | Train loss: 0.130 | Val loss: 0.807 | Gen: ethay ariway onnitioningcay isay orkingway\n","Epoch:  34 | Train loss: 0.133 | Val loss: 0.755 | Gen: ethay arway onditiongcay isway orkway\n","Epoch:  35 | Train loss: 0.120 | Val loss: 0.689 | Gen: ethay ariway onditioningcay issay orkingway\n","Epoch:  36 | Train loss: 0.078 | Val loss: 0.615 | Gen: ethay ariway onditiongcay isway orkwrway\n","Epoch:  37 | Train loss: 0.050 | Val loss: 0.684 | Gen: ethay ariway onditioningcay isay orkingway\n","Epoch:  38 | Train loss: 0.040 | Val loss: 0.628 | Gen: ethay ariway onditioningway isway orkwingway\n","Epoch:  39 | Train loss: 0.036 | Val loss: 0.699 | Gen: ethay ariway onditionigcay isay orkingway\n","Epoch:  40 | Train loss: 0.030 | Val loss: 0.637 | Gen: ethay ariway onditioningway iswasay orkingway\n","Epoch:  41 | Train loss: 0.029 | Val loss: 0.724 | Gen: ethay arirway onditionigcay isay orkingway\n","Epoch:  42 | Train loss: 0.026 | Val loss: 0.676 | Gen: ethay ariway onditioningcay iswasay orkingway\n","Epoch:  43 | Train loss: 0.024 | Val loss: 0.841 | Gen: ethay arirway onditioningcay isay orkingway\n","Epoch:  44 | Train loss: 0.030 | Val loss: 0.681 | Gen: ethay ariway onditioningcay isway orkwingway\n","Epoch:  45 | Train loss: 0.023 | Val loss: 0.804 | Gen: ethay ariway onditioningcay isay orkingway\n","Epoch:  46 | Train loss: 0.020 | Val loss: 0.759 | Gen: ethay ariway onditioningcay issay orkingway\n","Epoch:  47 | Train loss: 0.021 | Val loss: 0.893 | Gen: ethay ariway onditioningcay isay orkingway\n","Epoch:  48 | Train loss: 0.056 | Val loss: 0.857 | Gen: ethay airway onditioningcay isay orkingway\n","Epoch:  49 | Train loss: 0.092 | Val loss: 1.533 | Gen: etway arway onditiongway asay orkgway\n","Obtained lowest validation loss of: 0.6152846815741875\n","source:\t\tthe air conditioning is working \n","translated:\tetway arway onditiongway asay orkgway\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","trans64_args_s = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_small\",\n","    \"cuda\": True,\n","    \"nepochs\": 50,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 5e-4,\n","    \"early_stopping_patience\": 20,\n","    \"lr_decay\": 0.99,\n","    \"batch_size\": 64,\n","    \"hidden_size\": 64,  # Increased model size\n","    \"encoder_type\": \"transformer\",\n","    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n","    \"num_transformer_layers\": 3,\n","}\n","trans64_args_s.update(args_dict)\n","print_opts(trans64_args_s)\n","\n","trans64_encoder_s, trans64_decoder_s, trans64_losses_s = train(trans64_args_s)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, trans64_encoder_s, trans64_decoder_s, None, trans64_args_s\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"dardK4RWvUWV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647559713776,"user_tz":240,"elapsed":165226,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}},"outputId":"eb561d4d-6ea3-4df0-ea3e-db478f8a564f"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_large                        \n","                                   cuda: 1                                      \n","                                nepochs: 50                                     \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.0005                                 \n","                early_stopping_patience: 20                                     \n","                               lr_decay: 0.99                                   \n","                             batch_size: 512                                    \n","                            hidden_size: 64                                     \n","                           encoder_type: transformer                            \n","                           decoder_type: transformer                            \n","                 num_transformer_layers: 3                                      \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('merlin', 'erlinmay')\n","('lq', 'lqay')\n","('fiddle', 'iddlefay')\n","('applied', 'appliedway')\n","('multicast', 'ulticastmay')\n","Num unique word pairs: 22402\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 2.381 | Val loss: 1.903 | Gen: etay iwatwaway ongay isway oway\n","Epoch:   1 | Train loss: 1.666 | Val loss: 1.676 | Gen: eway arawaway ongongongongay isay orggway-ingway\n","Epoch:   2 | Train loss: 1.491 | Val loss: 1.616 | Gen: eway arwawawawawaway otiongay isay orway\n","Epoch:   3 | Train loss: 1.295 | Val loss: 1.598 | Gen: eway aray ongonay-inationgy isay origgway-ingway\n","Epoch:   4 | Train loss: 1.160 | Val loss: 1.526 | Gen: eway ay-ay-ay onggay-ongy-ongy isay orgway-ingway\n","Epoch:   5 | Train loss: 1.043 | Val loss: 1.469 | Gen: eway ay ongdgay-intingy isay orway-ingway\n","Epoch:   6 | Train loss: 0.965 | Val loss: 1.367 | Gen: ethay ay ondwayiongiongy isway orway-ingway\n","Epoch:   7 | Train loss: 0.875 | Val loss: 1.295 | Gen: ethay ay ondngingingway isway oworway\n","Epoch:   8 | Train loss: 0.802 | Val loss: 1.191 | Gen: ethay ay ondway-ingingongingi isway oway-ingwgwgway\n","Epoch:   9 | Train loss: 0.709 | Val loss: 1.166 | Gen: ethay irwayway ondingiongingway isway owringway\n","Epoch:  10 | Train loss: 0.631 | Val loss: 1.060 | Gen: eway airway ondingayingincay isway owringway\n","Epoch:  11 | Train loss: 0.565 | Val loss: 0.977 | Gen: ethay airway ondingontingscay isway orkingway\n","Epoch:  12 | Train loss: 0.511 | Val loss: 1.025 | Gen: ethay airway ondingiongcay isay owengway\n","Epoch:  13 | Train loss: 0.495 | Val loss: 0.991 | Gen: ethay airway ondongiongincay isway oroingway\n","Epoch:  14 | Train loss: 0.462 | Val loss: 0.943 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  15 | Train loss: 0.438 | Val loss: 0.858 | Gen: theway airway onditongiongcay isway orkingway\n","Epoch:  16 | Train loss: 0.374 | Val loss: 0.827 | Gen: ethay airway onditingongincay isway orkingway\n","Epoch:  17 | Train loss: 0.334 | Val loss: 0.942 | Gen: ethay airway onditingoncayingnway isway orkingnway\n","Epoch:  18 | Train loss: 0.359 | Val loss: 0.793 | Gen: thetay airway onditingcay iisway oringway\n","Epoch:  19 | Train loss: 0.296 | Val loss: 0.791 | Gen: ethay airway ondittingcingway isway orkinggway\n","Epoch:  20 | Train loss: 0.296 | Val loss: 0.823 | Gen: ethtay airway onditiongdngcay isway orkinggway\n","Epoch:  21 | Train loss: 0.290 | Val loss: 0.805 | Gen: eththay airway onditiongingntingnda isway owringgway\n","Epoch:  22 | Train loss: 0.245 | Val loss: 0.656 | Gen: ethay airway onditiongingcay issway orkingkway\n","Epoch:  23 | Train loss: 0.200 | Val loss: 0.684 | Gen: ethay irway onditionginingcay isway orkinggway\n","Epoch:  24 | Train loss: 0.211 | Val loss: 0.631 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  25 | Train loss: 0.184 | Val loss: 0.584 | Gen: ethay airway onditiongcay isway orkinggway\n","Epoch:  26 | Train loss: 0.201 | Val loss: 0.632 | Gen: ethay airway onditioingcay isway orkingway\n","Epoch:  27 | Train loss: 0.212 | Val loss: 0.891 | Gen: ehethay airwaway onditiongcingaycay isswawawawawawawwwww oringweway\n","Epoch:  28 | Train loss: 0.280 | Val loss: 0.753 | Gen: eththay away onditiongciningnay issway okringnway\n","Epoch:  29 | Train loss: 0.237 | Val loss: 0.893 | Gen: etthay aaway onditiongcay iway oway-ingway\n","Epoch:  30 | Train loss: 0.278 | Val loss: 0.591 | Gen: ethay arway onditiongingcay isway orkingway\n","Epoch:  31 | Train loss: 0.151 | Val loss: 0.457 | Gen: ehthay airway onditiongcay isway orkingway\n","Epoch:  32 | Train loss: 0.112 | Val loss: 0.502 | Gen: eehtay airway onditiongingcay isway orkingway\n","Epoch:  33 | Train loss: 0.110 | Val loss: 0.427 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  34 | Train loss: 0.095 | Val loss: 0.499 | Gen: eethay airway onditionongway isway orkingway\n","Epoch:  35 | Train loss: 0.096 | Val loss: 0.425 | Gen: ethay airway onditiongingcay isway orkingway\n","Epoch:  36 | Train loss: 0.075 | Val loss: 0.419 | Gen: eethay airway onditiongingcay isway orkingway\n","Epoch:  37 | Train loss: 0.070 | Val loss: 0.390 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  38 | Train loss: 0.058 | Val loss: 0.402 | Gen: eethay airway onditioningcay isway orkingway\n","Epoch:  39 | Train loss: 0.053 | Val loss: 0.383 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  40 | Train loss: 0.049 | Val loss: 0.409 | Gen: eethay airway onditioningcay isway orkingway\n","Epoch:  41 | Train loss: 0.051 | Val loss: 0.387 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  42 | Train loss: 0.045 | Val loss: 0.403 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  43 | Train loss: 0.041 | Val loss: 0.396 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  44 | Train loss: 0.038 | Val loss: 0.434 | Gen: eethay airway onditioningcay isway orkingway\n","Epoch:  45 | Train loss: 0.041 | Val loss: 0.402 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  46 | Train loss: 0.050 | Val loss: 0.715 | Gen: ethay airway onditioningcay iway orkingway\n","Epoch:  47 | Train loss: 0.139 | Val loss: 0.886 | Gen: ethay awirwway ondditinningcay isway orkingwrway\n","Epoch:  48 | Train loss: 0.303 | Val loss: 0.788 | Gen: ettay iraway onditiziongincay isway orkingway\n","Epoch:  49 | Train loss: 0.210 | Val loss: 0.539 | Gen: ethtay airway onditioningcay issway orkingway\n","Obtained lowest validation loss of: 0.3825042962920494\n","source:\t\tthe air conditioning is working \n","translated:\tethtay airway onditioningcay issway orkingway\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","trans64_args_l = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_large\",  # Increased data set size\n","    \"cuda\": True,\n","    \"nepochs\": 50,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 5e-4,\n","    \"early_stopping_patience\": 20,\n","    \"lr_decay\": 0.99,\n","    \"batch_size\": 512,\n","    \"hidden_size\": 64,  # Increased model size\n","    \"encoder_type\": \"transformer\",\n","    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n","    \"num_transformer_layers\": 3,\n","}\n","trans64_args_l.update(args_dict)\n","print_opts(trans64_args_l)\n","\n","trans64_encoder_l, trans64_decoder_l, trans64_losses_l = train(trans64_args_l)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, trans64_encoder_l, trans64_decoder_l, None, trans64_args_l\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"markdown","metadata":{"id":"pSSyiG39vVlN"},"source":["The following cell generates two loss plots. In the first plot, we compare the effects of increasing dataset size. In the second plot, we compare the effects of increasing model size. Include both plots in your report, and include your analysis of the results."]},{"cell_type":"code","execution_count":47,"metadata":{"id":"-Ql0pxrEvVP6","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1647559715137,"user_tz":240,"elapsed":1368,"user":{"displayName":"Lisa Yu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjS2p68WTmwgAZpH2CqXUIuSj3jBMeqRxwhhElXJg=s64","userId":"02835623436724771541"}},"outputId":"73bc0f41-d264-4189-9b53-625b7d60dd97"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}}],"source":["save_loss_comparison_by_dataset(\n","    trans32_losses_s,\n","    trans32_losses_l,\n","    trans64_losses_s,\n","    trans64_losses_l,\n","    trans32_args_s,\n","    trans32_args_l,\n","    trans64_args_s,\n","    trans64_args_l,\n","    \"trans_by_dataset\",\n",")\n","save_loss_comparison_by_hidden(\n","    trans32_losses_s,\n","    trans32_losses_l,\n","    trans64_losses_s,\n","    trans64_losses_l,\n","    trans32_args_s,\n","    trans32_args_l,\n","    trans64_args_s,\n","    trans64_args_l,\n","    \"trans_by_hidden\",\n",")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["TjPTaRB4mpCd","s9IS9B9-yUU5","9DaTdRNuUra7","4BIpGwANoQOg","pbvpn4MaV0I1","bRWfRdmVVjUl","0yh08KhgnA30","YDYMr7NclZdw","dCae1mOUlZrC","ecEq4TP2lZ4Z","TSDTbsydlaGI","RWwA6OGqlaTq","AJSafHSAmu_w","73_p8d5EmvOJ","vYPae08Io1Fi","xq7nhsEio1w-","unReAOrjo113","ZkjHbtvT6Qxs","B7gJLw5t_rnW","9tcpUFKqo2Oi","29ZjkXTNrUKb"],"name":"Copy of nmt_new.ipynb","provenance":[{"file_id":"https://github.com/uoft-csc413/2022/blob/master/assets/assignments/nmt.ipynb","timestamp":1647383571080}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}