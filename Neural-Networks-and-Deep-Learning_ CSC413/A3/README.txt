Neural Machine Traslation using RNN encoder/decoder model where each cell unit is a Gated Recurrent Unit cell (a simplified LSTM cell). Also analyzed performance compared to RNN with additive attention and with scaled dot product attention as well as Transformer with scaled dot product attention.

Finetuning BERT language models on sentence classification task.

Exploring the CLIP model to predict the caption of an image to learn better image representations.
